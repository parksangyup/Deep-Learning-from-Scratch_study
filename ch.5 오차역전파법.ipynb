{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "#곱셈 계층 MulLayer\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "    \n",
    "        return out\n",
    "    def backward(self,dout): # 상류에서 넘어온 미분(dout)에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘림\n",
    "        \n",
    "        dx = dout * self.y # x와 y를 바꾼다.\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "    \n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price,tax)\n",
    "\n",
    "print(price)\n",
    "\n",
    "#역전파\n",
    "dprice = 1 # 출력층의 미분값은 1\n",
    "dapple_price, tax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "#덧셈계층 AddLayer\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x,y):\n",
    "        out = x+y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout *1#덧셈노드는 그대로이기 때문에\n",
    "        dy = dout *1\n",
    "        return dx,dy\n",
    "    \n",
    "\n",
    "apple = 100\n",
    "apple_num =2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "#계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "#순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange,orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "#역전파 \n",
    "dprice = 1 #맨 끝에서 출발할땐 미분값이 1이기 때문에 \n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "#활성화 함수 계층 구현하기 \n",
    "# ReLU 계층!!!\n",
    "#신경망을 구성하는 층인 계층은 각각을 클래스 하나로 구현\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None###mask는 0보다 크면 False로 작으면 True로 형태로 배열 저장\n",
    "    def foward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "x = np.array([[1,-0.5],[-2,3]])\n",
    "print(x)\n",
    "\n",
    "mask = (x<=0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigmoid계층\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = 1 /(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout*(1-self.out)*self.out\n",
    "        return dx\n",
    "    ## 순전파의 출력을 out에다가 보관했다가 다시 역전파 계신시 그 값을 이용\n",
    "    ###즉 dout * y(1-y)!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "#배치용 Affine계층\n",
    "x_dot_W = np.array([[0,0,0],[10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "print(x_dot_W)\n",
    "\n",
    "print(x_dot_W + B)\n",
    "## 이렇게 되면 안됨 첫번째 열의 합이 더해져야함\n",
    "dy = np.array([[1,2,3],[4,5,6]])\n",
    "print(dy)\n",
    "\n",
    "db = np.sum(dy,axis=0)\n",
    "print(db)\n",
    "##이렇게 해야 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Affine 계층 클래스 \n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W)+self.B\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis = 0)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax-with-loss\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #손실\n",
    "        self.y = None #softmax 의 출력\n",
    "        self.t = None # 정답 레이블 (원핫벡터)\n",
    "        \n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y ,self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y-self.t)/batch_size##역전파는 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞계층으로 전파한다!!! \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two hidden layer (backpropagation) \n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b2:1.3970239658989137e-07\n",
      "W1:2.3401499778134436e-06\n",
      "W2:6.961034329968084e-09\n",
      "b1:1.5605516269185457e-05\n"
     ]
    }
   ],
   "source": [
    "#기울기 검증하기 \n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "#from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-17-aa41d25caa01>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-aa41d25caa01>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    loss_W = lambda W : self.loss(x,t)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#two-hidden layer\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,weight_init_std=0.01):\n",
    "        #가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size,hidden_size)#0과 1사이의 값을 랜덤하게 배열의 크기는 입력층 뉴런수 곱하기 은닉층 뉴런수\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)#은닉층의 사이즈 만큼 영인 배열 생성\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self,x): # 예측!!\n",
    "        W1,W2 = self.params[\"W1\"],self.params[\"W2\"]\n",
    "        b1,b2 = self.params['b1'],self.params[\"b2\"]\n",
    "        a1 = np.dot(x,W1)+b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2)+b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self,x,t): # 손실 함수 x는 입력, t는 정답 데이터\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "    def accuracy(self,x,t): # 예측은 정확도를 계산\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis = 1)# argmax 는 배열중에서 가장 큰 값의 위치를 반환\n",
    "        #axis 는  축 즉 배열의 차원 , rank는 축의 개수\n",
    "        t = np.argmax(t,axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    def numerical_gradient(self,x,t): r\n",
    "        loss_W = lambda W : self.loss(x,t)\n",
    "        \n",
    "        grads = {}#기울기를 딕셔너리로 저장\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12316666666666666 0.1235\n",
      "0.9042833333333333 0.9083\n",
      "0.9241166666666667 0.926\n",
      "0.9362666666666667 0.9352\n",
      "0.9457 0.9427\n",
      "0.9528666666666666 0.9478\n",
      "0.9576333333333333 0.9534\n",
      "0.9615333333333334 0.958\n",
      "0.9646 0.9604\n",
      "0.96725 0.962\n",
      "0.96985 0.9633\n",
      "0.97275 0.9674\n",
      "0.9746666666666667 0.9678\n",
      "0.9749833333333333 0.9687\n",
      "0.9775666666666667 0.9693\n",
      "0.9768666666666667 0.9674\n",
      "0.9791333333333333 0.9693\n"
     ]
    }
   ],
   "source": [
    "#오차역전파법을 사용한 학습 구현하기\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#데이터 읽어오기 \n",
    "(x_train,t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label =True)\n",
    "network = TwoLayerNet(input_size=784,hidden_size=50,output_size = 10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list = []\n",
    "test_acc_list=[]\n",
    "\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size,batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    #오차역전파법으로 기울기 구하기\n",
    "    grad = network.gradient(x_batch,t_batch)\n",
    "    \n",
    "    #갱신\n",
    "    for key in (\"W1\",\"b1\",\"W2\",\"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train,t_train)\n",
    "        test_acc = network.accuracy(x_test,t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
