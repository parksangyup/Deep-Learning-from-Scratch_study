{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and Y data \n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight') \n",
    "b = tf.Variable(tf.random_normal([1]), name='bias') # Our hypothesis XW+b \n",
    "#w 와 b를 Variable 이라함 이건 텐서플로의 변수임 텐서플로가 시작시키면\n",
    "#텐서플로가 자체적으로 수정시키는 변수라고 함 일반적인 변수랑은 다름\n",
    "#처음엔 w b 의 값을 모르니까 랜덤하게 줌 \n",
    "#값이 1나 랜덤하게 노멀하게\n",
    "hypothesis = x_train * W + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코스트 function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))#reduce_mean은 최소화시킨다.!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost/loss function \n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "\n",
    "# Minimize 이\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) \n",
    "train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.3580837 [1.1420975] [1.581053]\n",
      "20 0.249331 [0.533428] [1.2419323]\n",
      "40 0.19282033 [0.498034] [1.1583505]\n",
      "60 0.17481768 [0.51616645] [1.1015112]\n",
      "80 0.15876946 [0.5383851] [1.0495156]\n",
      "100 0.14419687 [0.56002986] [1.0001705]\n",
      "120 0.13096197 [0.58070207] [0.9531642]\n",
      "140 0.11894176 [0.60040706] [0.9083689]\n",
      "160 0.1080248 [0.6191865] [0.8656788]\n",
      "180 0.09810988 [0.63708323] [0.8249951]\n",
      "200 0.08910495 [0.654139] [0.7862234]\n",
      "220 0.08092653 [0.67039317] [0.7492738]\n",
      "240 0.07349876 [0.68588346] [0.71406054]\n",
      "260 0.06675277 [0.70064586] [0.6805023]\n",
      "280 0.060625926 [0.7147144] [0.64852124]\n",
      "300 0.0550614 [0.7281218] [0.618043]\n",
      "320 0.050007645 [0.7408991] [0.5889972]\n",
      "340 0.04541776 [0.75307596] [0.5613165]\n",
      "360 0.041249104 [0.7646805] [0.5349366]\n",
      "380 0.0374631 [0.7757396] [0.5097965]\n",
      "400 0.03402458 [0.786279] [0.48583794]\n",
      "420 0.030901669 [0.7963231] [0.46300536]\n",
      "440 0.028065383 [0.8058951] [0.44124582]\n",
      "460 0.025489442 [0.81501746] [0.42050892]\n",
      "480 0.023149898 [0.8237109] [0.40074643]\n",
      "500 0.0210251 [0.8319959] [0.38191283]\n",
      "520 0.019095339 [0.8398915] [0.36396432]\n",
      "540 0.01734271 [0.847416] [0.34685937]\n",
      "560 0.015750917 [0.8545869] [0.3305582]\n",
      "580 0.014305234 [0.8614208] [0.31502318]\n",
      "600 0.012992244 [0.86793345] [0.3002182]\n",
      "620 0.011799765 [0.87414014] [0.28610906]\n",
      "640 0.010716744 [0.88005507] [0.27266303]\n",
      "660 0.00973312 [0.885692] [0.25984886]\n",
      "680 0.008839765 [0.891064] [0.24763703]\n",
      "700 0.008028416 [0.8961836] [0.23599903]\n",
      "720 0.007291548 [0.90106267] [0.22490792]\n",
      "740 0.0066223014 [0.9057123] [0.21433805]\n",
      "760 0.0060144677 [0.9101435] [0.20426498]\n",
      "780 0.0054624453 [0.91436636] [0.19466528]\n",
      "800 0.0049610767 [0.91839075] [0.18551676]\n",
      "820 0.00450574 [0.92222613] [0.17679816]\n",
      "840 0.004092179 [0.92588127] [0.16848929]\n",
      "860 0.0037165927 [0.92936456] [0.16057096]\n",
      "880 0.0033754592 [0.9326841] [0.15302473]\n",
      "900 0.003065658 [0.93584776] [0.14583315]\n",
      "920 0.0027842738 [0.93886274] [0.13897951]\n",
      "940 0.0025287147 [0.9417359] [0.13244793]\n",
      "960 0.0022966187 [0.94447416] [0.12622334]\n",
      "980 0.0020858266 [0.94708365] [0.12029131]\n",
      "1000 0.0018943794 [0.9495706] [0.11463805]\n",
      "1020 0.0017205033 [0.95194054] [0.10925045]\n",
      "1040 0.0015625902 [0.95419925] [0.10411607]\n",
      "1060 0.0014191661 [0.9563517] [0.09922297]\n",
      "1080 0.0012889112 [0.95840305] [0.09455983]\n",
      "1100 0.0011706069 [0.96035796] [0.09011583]\n",
      "1120 0.0010631633 [0.9622209] [0.0858807]\n",
      "1140 0.0009655836 [0.9639964] [0.08184465]\n",
      "1160 0.0008769577 [0.9656884] [0.07799824]\n",
      "1180 0.000796468 [0.96730095] [0.07433261]\n",
      "1200 0.0007233662 [0.9688376] [0.07083927]\n",
      "1220 0.0006569733 [0.9703022] [0.06751007]\n",
      "1240 0.0005966713 [0.97169787] [0.06433733]\n",
      "1260 0.00054190593 [0.973028] [0.0613137]\n",
      "1280 0.0004921663 [0.9742956] [0.05843215]\n",
      "1300 0.00044699584 [0.9755036] [0.05568605]\n",
      "1320 0.00040596616 [0.9766548] [0.05306903]\n",
      "1340 0.00036870528 [0.97775203] [0.05057495]\n",
      "1360 0.00033486425 [0.9787975] [0.0481981]\n",
      "1380 0.00030412924 [0.9797941] [0.04593297]\n",
      "1400 0.0002762139 [0.98074365] [0.04377426]\n",
      "1420 0.00025086224 [0.9816486] [0.04171702]\n",
      "1440 0.00022783702 [0.98251104] [0.03975646]\n",
      "1460 0.00020692585 [0.983333] [0.03788805]\n",
      "1480 0.00018793256 [0.98411626] [0.03610744]\n",
      "1500 0.00017068327 [0.9848627] [0.03441055]\n",
      "1520 0.00015501771 [0.9855741] [0.03279338]\n",
      "1540 0.00014078977 [0.98625207] [0.03125221]\n",
      "1560 0.00012786705 [0.98689824] [0.02978347]\n",
      "1580 0.000116132134 [0.98751396] [0.02838374]\n",
      "1600 0.000105472085 [0.9881007] [0.02704979]\n",
      "1620 9.5790805e-05 [0.98866] [0.02577856]\n",
      "1640 8.7000044e-05 [0.98919284] [0.02456708]\n",
      "1660 7.901542e-05 [0.98970073] [0.02341253]\n",
      "1680 7.176132e-05 [0.9901849] [0.02231224]\n",
      "1700 6.517487e-05 [0.9906462] [0.02126359]\n",
      "1720 5.91937e-05 [0.9910857] [0.02026425]\n",
      "1740 5.3760203e-05 [0.99150467] [0.01931187]\n",
      "1760 4.8826263e-05 [0.9919039] [0.01840427]\n",
      "1780 4.4343342e-05 [0.9922844] [0.01753934]\n",
      "1800 4.0274128e-05 [0.992647] [0.01671506]\n",
      "1820 3.6577345e-05 [0.9929926] [0.01592951]\n",
      "1840 3.3220247e-05 [0.99332196] [0.01518085]\n",
      "1860 3.0170486e-05 [0.9936358] [0.01446741]\n",
      "1880 2.7402246e-05 [0.99393487] [0.01378749]\n",
      "1900 2.4886867e-05 [0.9942199] [0.01313953]\n",
      "1920 2.2602317e-05 [0.9944915] [0.01252203]\n",
      "1940 2.0527996e-05 [0.9947503] [0.01193358]\n",
      "1960 1.8644057e-05 [0.9949971] [0.01137276]\n",
      "1980 1.6932663e-05 [0.9952322] [0.0108383]\n",
      "2000 1.5378393e-05 [0.99545634] [0.01032893]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 2.82329 [ 2.12867713] [-0.85235667]\\n20 0.190351 [ 1.53392804] [-1.05059612]\\n40 0.151357 [ 1.45725465] [-1.02391243]\\n...\\n1960 1.46397e-05 [ 1.004444] [-0.01010205]\\n1980 1.32962e-05 [ 1.00423515] [-0.00962736]\\n2000 1.20761e-05 [ 1.00403607] [-0.00917497]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 2 Linear Regression\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(2001):\n",
    "        _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val, b_val)\n",
    "\n",
    "# Learns best fit W:[ 1.],  b:[ 0.]\n",
    "#차례대로 코스트 w b 차례대로 나옴 \n",
    "#시행을 할 수록 코스트가 점차 작게 수렴됨\n",
    "\"\"\"\n",
    "0 2.82329 [ 2.12867713] [-0.85235667]\n",
    "20 0.190351 [ 1.53392804] [-1.05059612]\n",
    "40 0.151357 [ 1.45725465] [-1.02391243]\n",
    "...\n",
    "1960 1.46397e-05 [ 1.004444] [-0.01010205]\n",
    "1980 1.32962e-05 [ 1.00423515] [-0.00962736]\n",
    "2000 1.20761e-05 [ 1.00403607] [-0.00917497]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4106206 [0.35157153] [1.0152376]\n",
      "20 0.15760784 [0.52709705] [1.0313337]\n",
      "40 0.14118971 [0.56313336] [0.9889406]\n",
      "60 0.1282131 [0.58497965] [0.94304264]\n",
      "80 0.116445005 [0.6046093] [0.89877814]\n",
      "100 0.105757214 [0.62320316] [0.8565441]\n",
      "120 0.09605038 [0.6409125] [0.81629014]\n",
      "140 0.08723447 [0.6577883] [0.7779275]\n",
      "160 0.07922775 [0.6738711] [0.74136776]\n",
      "180 0.0719559 [0.68919796] [0.7065261]\n",
      "200 0.06535149 [0.70380455] [0.67332196]\n",
      "220 0.059353266 [0.7177246] [0.6416783]\n",
      "240 0.053905636 [0.7309905] [0.61152184]\n",
      "260 0.048957933 [0.743633] [0.5827826]\n",
      "280 0.044464383 [0.75568116] [0.55539405]\n",
      "300 0.040383253 [0.7671632] [0.52929264]\n",
      "320 0.036676753 [0.7781057] [0.50441784]\n",
      "340 0.033310402 [0.788534] [0.48071203]\n",
      "360 0.030253038 [0.7984721] [0.4581203]\n",
      "380 0.027476309 [0.80794317] [0.43659037]\n",
      "400 0.0249544 [0.81696904] [0.41607222]\n",
      "420 0.022664003 [0.8255709] [0.39651838]\n",
      "440 0.020583816 [0.8337684] [0.37788346]\n",
      "460 0.018694533 [0.8415807] [0.3601243]\n",
      "480 0.016978674 [0.84902585] [0.34319976]\n",
      "500 0.015420315 [0.85612106] [0.32707065]\n",
      "520 0.014004963 [0.8628829] [0.31169948]\n",
      "540 0.012719523 [0.869327] [0.2970506]\n",
      "560 0.011552084 [0.87546813] [0.2830903]\n",
      "580 0.01049177 [0.88132066] [0.26978615]\n",
      "600 0.009528815 [0.88689816] [0.2571072]\n",
      "620 0.0086542 [0.8922135] [0.24502407]\n",
      "640 0.00785989 [0.8972791] [0.2335088]\n",
      "660 0.00713848 [0.9021066] [0.22253475]\n",
      "680 0.0064832815 [0.9067073] [0.21207641]\n",
      "700 0.005888215 [0.9110916] [0.20210958]\n",
      "720 0.0053477692 [0.91527] [0.19261117]\n",
      "740 0.00485694 [0.919252] [0.18355922]\n",
      "760 0.0044111554 [0.9230469] [0.17493258]\n",
      "780 0.0040062745 [0.92666334] [0.1667114]\n",
      "800 0.0036385607 [0.9301099] [0.15887658]\n",
      "820 0.0033046023 [0.9333945] [0.15141]\n",
      "840 0.0030012932 [0.9365247] [0.1442943]\n",
      "860 0.0027258312 [0.9395077] [0.13751306]\n",
      "880 0.0024756342 [0.9423507] [0.13105042]\n",
      "900 0.0022484127 [0.94505996] [0.12489157]\n",
      "920 0.0020420405 [0.94764197] [0.11902212]\n",
      "940 0.0018546167 [0.9501026] [0.11342852]\n",
      "960 0.0016843969 [0.9524476] [0.10809778]\n",
      "980 0.0015297929 [0.9546824] [0.10301758]\n",
      "1000 0.0013893765 [0.95681214] [0.09817611]\n",
      "1020 0.0012618642 [0.95884174] [0.09356224]\n",
      "1040 0.0011460393 [0.9607761] [0.08916518]\n",
      "1060 0.0010408534 [0.9626194] [0.08497474]\n",
      "1080 0.00094532134 [0.9643762] [0.08098125]\n",
      "1100 0.0008585537 [0.96605045] [0.07717542]\n",
      "1120 0.0007797534 [0.9676459] [0.07354844]\n",
      "1140 0.00070818356 [0.96916646] [0.07009192]\n",
      "1160 0.00064318377 [0.97061557] [0.06679781]\n",
      "1180 0.00058414653 [0.97199655] [0.06365854]\n",
      "1200 0.00053053297 [0.9733126] [0.0606668]\n",
      "1220 0.00048183682 [0.9745668] [0.05781567]\n",
      "1240 0.0004376121 [0.97576207] [0.05509853]\n",
      "1260 0.00039744665 [0.9769011] [0.05250911]\n",
      "1280 0.00036096864 [0.97798663] [0.05004141]\n",
      "1300 0.0003278365 [0.9790212] [0.04768967]\n",
      "1320 0.0002977491 [0.98000705] [0.04544846]\n",
      "1340 0.0002704178 [0.9809467] [0.04331256]\n",
      "1360 0.00024559823 [0.98184216] [0.04127702]\n",
      "1380 0.0002230573 [0.9826955] [0.03933716]\n",
      "1400 0.00020258238 [0.9835088] [0.03748843]\n",
      "1420 0.0001839905 [0.9842838] [0.0357266]\n",
      "1440 0.00016710405 [0.98502237] [0.0340476]\n",
      "1460 0.00015176531 [0.9857263] [0.03244752]\n",
      "1480 0.00013783661 [0.9863971] [0.03092261]\n",
      "1500 0.00012518636 [0.98703635] [0.02946937]\n",
      "1520 0.00011369546 [0.9876456] [0.02808443]\n",
      "1540 0.000103260536 [0.9882262] [0.02676458]\n",
      "1560 9.3781746e-05 [0.98877954] [0.02550677]\n",
      "1580 8.517524e-05 [0.9893068] [0.02430805]\n",
      "1600 7.7357276e-05 [0.9898093] [0.02316573]\n",
      "1620 7.025734e-05 [0.9902883] [0.02207709]\n",
      "1640 6.380803e-05 [0.99074477] [0.02103948]\n",
      "1660 5.7952493e-05 [0.9911797] [0.02005067]\n",
      "1680 5.2633157e-05 [0.99159425] [0.01910833]\n",
      "1700 4.7801892e-05 [0.99198925] [0.01821031]\n",
      "1720 4.341437e-05 [0.9923657] [0.0173545]\n",
      "1740 3.943003e-05 [0.99272454] [0.0165389]\n",
      "1760 3.5810586e-05 [0.99306643] [0.01576162]\n",
      "1780 3.25234e-05 [0.9933923] [0.01502086]\n",
      "1800 2.9538673e-05 [0.9937028] [0.01431495]\n",
      "1820 2.6827389e-05 [0.99399877] [0.0136422]\n",
      "1840 2.4365267e-05 [0.99428076] [0.01300106]\n",
      "1860 2.2128255e-05 [0.9945496] [0.01239006]\n",
      "1880 2.009732e-05 [0.99480575] [0.01180777]\n",
      "1900 1.8253011e-05 [0.9950499] [0.01125283]\n",
      "1920 1.657738e-05 [0.99528253] [0.01072396]\n",
      "1940 1.5055851e-05 [0.9955042] [0.01021998]\n",
      "1960 1.3673883e-05 [0.9957155] [0.00973967]\n",
      "1980 1.24194e-05 [0.99591684] [0.00928195]\n",
      "2000 1.1279374e-05 [0.9961087] [0.00884575]\n",
      "[4.9893894]\n",
      "[2.4991176]\n",
      "[1.5030088 3.4952264]\n",
      "0 1.21626 [1.0624341] [0.03090231]\n",
      "20 0.16294633 [1.2601887] [0.15682572]\n",
      "40 0.1422839 [1.2440603] [0.21884748]\n",
      "60 0.12425755 [1.2280806] [0.27655604]\n",
      "80 0.10851507 [1.2131435] [0.33048406]\n",
      "100 0.094767 [1.1991847] [0.38088033]\n",
      "120 0.082760766 [1.18614] [0.42797595]\n",
      "140 0.072275534 [1.1739494] [0.47198737]\n",
      "160 0.06311882 [1.1625574] [0.51311636]\n",
      "180 0.05512213 [1.1519113] [0.55155176]\n",
      "200 0.048138533 [1.1419625] [0.5874702]\n",
      "220 0.042039745 [1.1326653] [0.6210361]\n",
      "240 0.036713652 [1.1239771] [0.65240383]\n",
      "260 0.03206225 [1.1158576] [0.6817174]\n",
      "280 0.028000232 [1.1082699] [0.70911103]\n",
      "300 0.0244528 [1.1011792] [0.73471063]\n",
      "320 0.021354781 [1.0945529] [0.75863385]\n",
      "340 0.018649297 [1.0883605] [0.78099024]\n",
      "360 0.016286556 [1.0825738] [0.80188245]\n",
      "380 0.014223183 [1.077166] [0.8214064]\n",
      "400 0.01242123 [1.0721124] [0.83965164]\n",
      "420 0.0108475415 [1.0673896] [0.856702]\n",
      "440 0.009473243 [1.0629762] [0.8726358]\n",
      "460 0.0082730595 [1.0588518] [0.887526]\n",
      "480 0.0072249323 [1.0549977] [0.901441]\n",
      "500 0.0063095884 [1.0513958] [0.9144448]\n",
      "520 0.0055102073 [1.0480298] [0.926597]\n",
      "540 0.004812096 [1.0448843] [0.9379533]\n",
      "560 0.0042024357 [1.0419447] [0.94856584]\n",
      "580 0.0036700158 [1.0391978] [0.9584835]\n",
      "600 0.0032050523 [1.0366307] [0.96775156]\n",
      "620 0.002799002 [1.0342318] [0.97641265]\n",
      "640 0.002444384 [1.0319898] [0.9845065]\n",
      "660 0.0021346959 [1.0298947] [0.9920702]\n",
      "680 0.001864247 [1.0279369] [0.99913865]\n",
      "700 0.0016280562 [1.0261073] [1.0057442]\n",
      "720 0.0014218001 [1.0243976] [1.0119169]\n",
      "740 0.001241674 [1.0227997] [1.0176854]\n",
      "760 0.0010843603 [1.0213066] [1.0230763]\n",
      "780 0.00094697 [1.0199112] [1.0281143]\n",
      "800 0.00082699367 [1.0186071] [1.0328223]\n",
      "820 0.00072222395 [1.0173885] [1.0372219]\n",
      "840 0.00063071813 [1.0162497] [1.0413333]\n",
      "860 0.00055081234 [1.0151855] [1.0451754]\n",
      "880 0.00048102904 [1.014191] [1.0487659]\n",
      "900 0.00042009074 [1.0132617] [1.0521212]\n",
      "920 0.00036686493 [1.0123931] [1.0552568]\n",
      "940 0.00032038777 [1.0115814] [1.058187]\n",
      "960 0.0002798015 [1.0108231] [1.0609252]\n",
      "980 0.0002443507 [1.0101141] [1.0634843]\n",
      "1000 0.00021339019 [1.0094517] [1.0658758]\n",
      "1020 0.00018635496 [1.0088328] [1.0681108]\n",
      "1040 0.00016274708 [1.0082543] [1.070199]\n",
      "1060 0.00014212722 [1.0077138] [1.0721508]\n",
      "1080 0.00012411707 [1.0072085] [1.0739748]\n",
      "1100 0.00010839368 [1.0067364] [1.0756793]\n",
      "1120 9.466165e-05 [1.0062952] [1.0772722]\n",
      "1140 8.266811e-05 [1.005883] [1.0787605]\n",
      "1160 7.2196926e-05 [1.0054978] [1.0801512]\n",
      "1180 6.3051586e-05 [1.0051378] [1.0814509]\n",
      "1200 5.5062188e-05 [1.0048013] [1.0826657]\n",
      "1220 4.808817e-05 [1.0044868] [1.0838009]\n",
      "1240 4.1993706e-05 [1.0041931] [1.0848619]\n",
      "1260 3.667424e-05 [1.0039184] [1.0858533]\n",
      "1280 3.2028493e-05 [1.0036618] [1.0867796]\n",
      "1300 2.7971753e-05 [1.003422] [1.0876454]\n",
      "1320 2.442672e-05 [1.0031979] [1.0884545]\n",
      "1340 2.133301e-05 [1.0029885] [1.0892104]\n",
      "1360 1.863005e-05 [1.0027927] [1.0899173]\n",
      "1380 1.6269161e-05 [1.0026098] [1.0905776]\n",
      "1400 1.4208704e-05 [1.0024389] [1.0911946]\n",
      "1420 1.2407909e-05 [1.0022792] [1.0917714]\n",
      "1440 1.0835919e-05 [1.0021299] [1.0923103]\n",
      "1460 9.46304e-06 [1.0019903] [1.092814]\n",
      "1480 8.26415e-06 [1.00186] [1.0932845]\n",
      "1500 7.2173048e-06 [1.0017383] [1.0937243]\n",
      "1520 6.3024863e-06 [1.0016245] [1.0941353]\n",
      "1540 5.503861e-06 [1.0015181] [1.0945194]\n",
      "1560 4.8069214e-06 [1.0014187] [1.0948782]\n",
      "1580 4.198159e-06 [1.0013257] [1.0952135]\n",
      "1600 3.6662138e-06 [1.001239] [1.0955269]\n",
      "1620 3.2019593e-06 [1.0011579] [1.0958198]\n",
      "1640 2.7964184e-06 [1.0010821] [1.0960935]\n",
      "1660 2.4421242e-06 [1.0010111] [1.0963494]\n",
      "1680 2.1329683e-06 [1.0009451] [1.0965883]\n",
      "1700 1.8626731e-06 [1.0008832] [1.0968117]\n",
      "1720 1.6266138e-06 [1.0008253] [1.0970204]\n",
      "1740 1.420712e-06 [1.0007713] [1.0972155]\n",
      "1760 1.2406985e-06 [1.0007207] [1.0973979]\n",
      "1780 1.0832589e-06 [1.0006735] [1.0975684]\n",
      "1800 9.463014e-07 [1.0006295] [1.0977275]\n",
      "1820 8.265744e-07 [1.0005882] [1.0978763]\n",
      "1840 7.2171895e-07 [1.0005497] [1.0980153]\n",
      "1860 6.30444e-07 [1.0005137] [1.0981451]\n",
      "1880 5.5056637e-07 [1.00048] [1.0982664]\n",
      "1900 4.810413e-07 [1.0004487] [1.0983797]\n",
      "1920 4.2012903e-07 [1.0004196] [1.0984857]\n",
      "1940 3.6691102e-07 [1.0003921] [1.0985849]\n",
      "1960 3.2046532e-07 [1.0003663] [1.0986775]\n",
      "1980 2.799226e-07 [1.0003424] [1.0987638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2.4453325e-07 [1.0003201] [1.0988448]\n",
      "[6.100445]\n",
      "[3.599645]\n",
      "[2.599325 4.599965]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Try to find values for W and b to compute Y = W * X + b\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "X = tf.placeholder(tf.float32, shape=[None])#shape none은 일차원 \n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "#x,y값을 플레이스 홀더를 사용해서 넘길 수 있다.\n",
    "#그래프를 넣을때마다 알아서 유동적으로 바뀌게 \n",
    "\n",
    "\n",
    "\n",
    "# Our hypothesis is X * W + b\n",
    "hypothesis = X * W + b\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Fit the line\n",
    "    for step in range(2001):\n",
    "        _, cost_val, W_val, b_val = sess.run(\n",
    "            [train, cost, W, b], feed_dict={X: [1, 2, 3], Y: [1, 2, 3]}\n",
    "        )\n",
    "        if step % 20 == 0:##20번에 한번식 출력을 해보는것\n",
    "            print(step, cost_val, W_val, b_val)\n",
    "\n",
    "    # Testing our model\n",
    "    print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "    print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))\n",
    "\n",
    "    # Learns best fit W:[ 1.],  b:[ 0]\n",
    "    \"\"\"##=코스트는 영으로 수렴\n",
    "    0 3.5240757 [2.2086694] [-0.8204183]\n",
    "    20 0.19749963 [1.5425726] [-1.0498911]\n",
    "    ...\n",
    "    1980 1.3360998e-05 [1.0042454] [-0.00965055]\n",
    "    2000 1.21343355e-05 [1.0040458] [-0.00919707]\n",
    "    [5.0110054]\n",
    "    [2.500915]\n",
    "    [1.4968792 3.5049512]\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the line with new training data\n",
    "    for step in range(2001):##w가 1이 되고 b = 1이되면 \n",
    "        _, cost_val, W_val, b_val = sess.run(\n",
    "            [train, cost, W, b],\n",
    "            feed_dict={X: [1, 2, 3, 4, 5], Y: [2.1, 3.1, 4.1, 5.1, 6.1]},\n",
    "        )\n",
    "        if step % 20 == 0:\n",
    "            print(step, cost_val, W_val, b_val)\n",
    "\n",
    "    # Testing our model\n",
    "    print(sess.run(hypothesis, feed_dict={X: [5]}))\n",
    "    print(sess.run(hypothesis, feed_dict={X: [2.5]}))\n",
    "    print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))#두개를 동시에 먹일떄\n",
    "                               ## 3개 다 비슷하게 학습이됨\n",
    "    # Learns best fit W:[ 1.],  b:[ 1.1]\n",
    "    \"\"\"\n",
    "    0 1.2035878 [1.0040361] [-0.00917497]\n",
    "    20 0.16904518 [1.2656431] [0.13599995]\n",
    "    ...\n",
    "    1980 2.9042917e-07 [1.00035] [1.0987366]\n",
    "    2000 2.5372992e-07 [1.0003271] [1.0988194]\n",
    "    [6.1004534]\n",
    "    [3.5996385]\n",
    "    [2.5993123 4.599964 ]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
