{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlibtensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9388b50f70c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Lab 3 Minimizing Cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlibtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlibtensor'"
     ]
    }
   ],
   "source": [
    "#코스트가 어떻게 생겼는지 보자\n",
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "import matplotlibtensor\n",
    "import tensorflow as tf\n",
    "\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "W = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "##H(X)\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "##C(W)\n",
    "# Variables for plotting cost function\n",
    "W_history = []\n",
    "cost_history = []\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    for i in range(-30, 50): #그래프를 -30,50까지 이동시켜가면서\n",
    "        curr_W = i * 0.1# 그래플을 0.1간격으로 짧게 움직이겠다. \n",
    "        curr_cost = sess.run(cost, feed_dict={W: curr_W})\n",
    "\n",
    "        W_history.append(curr_W)\n",
    "        cost_history.append(curr_cost)###그걸 리스트에 넎어준다.\n",
    "#세션을 열어 변수를 초기화시키고\n",
    "# Show the cost function\n",
    "plt.plot(W_history, cost_history)\n",
    "plt.show()\n",
    "#여기선 w가 1인부분이 최적으로 나옴 즉 w가 1이될때까지 찾아야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "##기울기 가 플러스라면 w는 -방향으로 움직여야한다,\n",
    "##기울기 가 마이너스라면 w는 +방향으로 움직여야 한다.\n",
    "##그래서 현재 W값에다가 기울기를 뺴주는것 !!\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "# Try to find values for W and b to compute y_data = W * x_data\n",
    "# We know that W should be 1\n",
    "# But let's use TensorFlow to figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Our hypothesis for linear model X * W\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent using derivative: W -= learning_rate * derivative\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((W * X - Y) * X)\n",
    "descent = W - learning_rate * gradient ##새로운 w 기울기 !!\n",
    "update = W.assign(descent)### 이걸 assign해야한다,!! \n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:###그래프이기 때문에 세션을 열고 변수를 초기화 시킴\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(21):\n",
    "        _, cost_val, W_val = sess.run(###업데이트를 실행\n",
    "            [update, cost, W], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "\"\"\"##스텝, 코스트, W\n",
    "0 1.93919 [ 1.64462376]\n",
    "1 0.551591 [ 1.34379935]\n",
    "2 0.156897 [ 1.18335962]\n",
    "3 0.0446285 [ 1.09779179]\n",
    "4 0.0126943 [ 1.05215561]\n",
    "5 0.00361082 [ 1.0278163]\n",
    "6 0.00102708 [ 1.01483536]\n",
    "7 0.000292144 [ 1.00791216]\n",
    "8 8.30968e-05 [ 1.00421977]\n",
    "9 2.36361e-05 [ 1.00225055]\n",
    "10 6.72385e-06 [ 1.00120032]\n",
    "11 1.91239e-06 [ 1.00064015]\n",
    "12 5.43968e-07 [ 1.00034142]\n",
    "13 1.54591e-07 [ 1.00018203]\n",
    "14 4.39416e-08 [ 1.00009704]\n",
    "15 1.24913e-08 [ 1.00005174]\n",
    "16 3.5322e-09 [ 1.00002754]\n",
    "17 9.99824e-10 [ 1.00001466]\n",
    "18 2.88878e-10 [ 1.00000787]\n",
    "19 8.02487e-11 [ 1.00000417]\n",
    "20 2.34053e-11 [ 1.00000226]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.0)###초기값 설정\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)#####이렇게 하면 텐서플로우가 알아서 코스트를 미분해줌!@@@\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, W_val = sess.run([train, W])\n",
    "        print(step, W_val)\n",
    "\n",
    "\"\"\"#학습을 할수록 초기값 5에서 빠르게 내려가서 1로 수렴함!!\n",
    "0 5.0\n",
    "1 1.2666664\n",
    "2 1.0177778\n",
    "3 1.0011852\n",
    "4 1.000079\n",
    "...\n",
    "97 1.0\n",
    "98 1.0\n",
    "99 1.0\n",
    "100 1.0\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 37.333332 [(37.333336, 4.6266665)]\n",
      "1 33.84889 [(33.84889, 4.2881775)]\n",
      "2 30.689657 [(30.689657, 3.9812808)]\n",
      "3 27.825287 [(27.825287, 3.703028)]\n",
      "4 25.228262 [(25.228262, 3.4507453)]\n",
      "5 22.873621 [(22.873623, 3.2220092)]\n",
      "6 20.738752 [(20.73875, 3.0146217)]\n",
      "7 18.803137 [(18.803137, 2.8265903)]\n",
      "8 17.048176 [(17.048176, 2.6561086)]\n",
      "9 15.457013 [(15.457014, 2.5015385)]\n",
      "10 14.014359 [(14.01436, 2.361395)]\n",
      "11 12.706352 [(12.706352, 2.2343314)]\n",
      "12 11.520427 [(11.520427, 2.119127)]\n",
      "13 10.445186 [(10.445185, 2.0146751)]\n",
      "14 9.470302 [(9.470302, 1.9199722)]\n",
      "15 8.586407 [(8.586407, 1.8341081)]\n",
      "16 7.785009 [(7.785009, 1.756258)]\n",
      "17 7.0584083 [(7.0584083, 1.685674)]\n",
      "18 6.399624 [(6.399624, 1.6216778)]\n",
      "19 5.8023257 [(5.8023252, 1.5636545)]\n",
      "20 5.260776 [(5.260776, 1.5110468)]\n",
      "21 4.7697697 [(4.7697697, 1.4633491)]\n",
      "22 4.324591 [(4.324591, 1.4201032)]\n",
      "23 3.9209633 [(3.9209635, 1.3808936)]\n",
      "24 3.5550067 [(3.5550067, 1.3453435)]\n",
      "25 3.2232056 [(3.2232056, 1.3131114)]\n",
      "26 2.9223735 [(2.9223735, 1.2838877)]\n",
      "27 2.6496189 [(2.6496186, 1.2573916)]\n",
      "28 2.4023216 [(2.4023216, 1.2333684)]\n",
      "29 2.178105 [(2.178105, 1.2115873)]\n",
      "30 1.9748148 [(1.9748147, 1.1918392)]\n",
      "31 1.7904993 [(1.7904994, 1.1739342)]\n",
      "32 1.623386 [(1.6233861, 1.1577003)]\n",
      "33 1.4718695 [(1.4718695, 1.1429816)]\n",
      "34 1.3344955 [(1.3344957, 1.1296366)]\n",
      "35 1.2099417 [(1.2099419, 1.1175373)]\n",
      "36 1.0970144 [(1.0970144, 1.1065671)]\n",
      "37 0.9946267 [(0.9946267, 1.0966209)]\n",
      "38 0.90179497 [(0.901795, 1.087603)]\n",
      "39 0.8176275 [(0.81762755, 1.0794266)]\n",
      "40 0.7413151 [(0.7413151, 1.0720135)]\n",
      "41 0.67212623 [(0.67212623, 1.0652922)]\n",
      "42 0.609394 [(0.609394, 1.0591983)]\n",
      "43 0.5525169 [(0.5525169, 1.0536731)]\n",
      "44 0.50094914 [(0.50094914, 1.0486636)]\n",
      "45 0.45419374 [(0.45419377, 1.0441216)]\n",
      "46 0.41180158 [(0.41180158, 1.0400037)]\n",
      "47 0.37336722 [(0.37336725, 1.03627)]\n",
      "48 0.33851996 [(0.33852, 1.0328848)]\n",
      "49 0.30692515 [(0.30692515, 1.0298156)]\n",
      "50 0.27827826 [(0.2782783, 1.0270327)]\n",
      "51 0.25230527 [(0.25230527, 1.0245097)]\n",
      "52 0.2287569 [(0.2287569, 1.022222)]\n",
      "53 0.20740573 [(0.20740573, 1.020148)]\n",
      "54 0.18804836 [(0.18804836, 1.0182675)]\n",
      "55 0.17049654 [(0.17049655, 1.0165626)]\n",
      "56 0.15458433 [(0.15458433, 1.0150168)]\n",
      "57 0.14015675 [(0.14015675, 1.0136153)]\n",
      "58 0.12707591 [(0.12707591, 1.0123445)]\n",
      "59 0.11521538 [(0.11521538, 1.0111923)]\n",
      "60 0.10446167 [(0.10446167, 1.0101477)]\n",
      "61 0.09471202 [(0.09471202, 1.0092006)]\n",
      "62 0.08587202 [(0.08587202, 1.0083419)]\n",
      "63 0.07785805 [(0.07785805, 1.0075634)]\n",
      "64 0.07059129 [(0.07059129, 1.0068574)]\n",
      "65 0.06400236 [(0.06400236, 1.0062174)]\n",
      "66 0.05802846 [(0.05802846, 1.005637)]\n",
      "67 0.052612226 [(0.052612226, 1.005111)]\n",
      "68 0.047702473 [(0.047702473, 1.0046339)]\n",
      "69 0.043249767 [(0.043249767, 1.0042014)]\n",
      "70 0.03921318 [(0.03921318, 1.0038093)]\n",
      "71 0.035553534 [(0.035553537, 1.0034539)]\n",
      "72 0.032236177 [(0.03223618, 1.0031315)]\n",
      "73 0.029227654 [(0.029227655, 1.0028392)]\n",
      "74 0.02649951 [(0.02649951, 1.0025742)]\n",
      "75 0.024025917 [(0.024025917, 1.002334)]\n",
      "76 0.021783749 [(0.02178375, 1.0021162)]\n",
      "77 0.01975123 [(0.019751232, 1.0019187)]\n",
      "78 0.017907381 [(0.017907381, 1.0017396)]\n",
      "79 0.016236702 [(0.016236704, 1.0015773)]\n",
      "80 0.014720838 [(0.014720838, 1.00143)]\n",
      "81 0.01334699 [(0.013346991, 1.0012965)]\n",
      "82 0.012100856 [(0.012100856, 1.0011755)]\n",
      "83 0.010971785 [(0.010971785, 1.0010659)]\n",
      "84 0.0099481745 [(0.0099481745, 1.0009663)]\n",
      "85 0.009018898 [(0.009018898, 1.0008761)]\n",
      "86 0.008176883 [(0.008176884, 1.0007943)]\n",
      "87 0.007413149 [(0.007413149, 1.0007201)]\n",
      "88 0.006721576 [(0.006721576, 1.0006529)]\n",
      "89 0.0060940585 [(0.0060940585, 1.000592)]\n",
      "90 0.005525271 [(0.0055252714, 1.0005368)]\n",
      "91 0.0050098896 [(0.0050098896, 1.0004867)]\n",
      "92 0.004542589 [(0.004542589, 1.0004413)]\n",
      "93 0.0041189194 [(0.0041189194, 1.0004001)]\n",
      "94 0.0037339528 [(0.003733953, 1.0003628)]\n",
      "95 0.0033854644 [(0.0033854644, 1.0003289)]\n",
      "96 0.0030694802 [(0.0030694804, 1.0002983)]\n",
      "97 0.0027837753 [(0.0027837753, 1.0002704)]\n",
      "98 0.0025234222 [(0.0025234222, 1.0002451)]\n",
      "99 0.0022875469 [(0.0022875469, 1.0002222)]\n",
      "100 0.0020739238 [(0.0020739238, 1.0002015)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 37.333332 [(37.333336, 5.0)]\\n1 33.84889 [(33.84889, 4.6266665)]\\n2 30.689657 [(30.689657, 4.2881775)]\\n3 27.825289 [(27.825289, 3.981281)]\\n...\\n97 0.0027837753 [(0.0027837753, 1.0002983)]\\n98 0.0025234222 [(0.0025234222, 1.0002704)]\\n99 0.0022875469 [(0.0022875469, 1.0002451)]\\n100 0.0020739238 [(0.0020739238, 1.0002222)]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 3 Minimizing Cost\n",
    "# This is optional\n",
    "##추가적인 강의 나중에 그레디언트를 임의로 수정하고 싶을때 사용\n",
    "##\n",
    "import tensorflow as tf\n",
    "\n",
    "# tf Graph Input\n",
    "X = [1, 2, 3]\n",
    "Y = [1, 2, 3]\n",
    "\n",
    "# Set wrong model weights\n",
    "W = tf.Variable(5.)\n",
    "\n",
    "# Linear model\n",
    "hypothesis = X * W\n",
    "\n",
    "# Manual gradient\n",
    "gradient = tf.reduce_mean((W * X - Y) * X) * 2##원래는 이거랑 맞음\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize: Gradient Descent Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)###여기까진 비슷함\n",
    "\n",
    "# Get gradients\n",
    "gvs = optimizer.compute_gradients(cost)######옵티마이즈에서 그래디언트를 계산해라 그걸 받아와서 \n",
    "#compute(loss, var_list)\n",
    "#minimize(loss, step, var_list)여기서 var_list는 안정해주면 입력된 가중치 모두 학습\n",
    "#하지만 특정 가중치만 학습시키고 싶으면 var_list에 입력\n",
    "\n",
    "# Optional: modify gradient if necessary\n",
    "# gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "\n",
    "# Apply gradients\n",
    "apply_gradients = optimizer.apply_gradients(gvs)#다시 할당함 즉 gvs를 수정해가면서 할 수 있음\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        gradient_val, gvs_val, _ = sess.run([gradient, gvs, apply_gradients])\n",
    "        print(step, gradient_val, gvs_val)\n",
    "\n",
    "'''\n",
    "0 37.333332 [(37.333336, 5.0)]\n",
    "1 33.84889 [(33.84889, 4.6266665)]\n",
    "2 30.689657 [(30.689657, 4.2881775)]\n",
    "3 27.825289 [(27.825289, 3.981281)]\n",
    "...\n",
    "97 0.0027837753 [(0.0027837753, 1.0002983)]\n",
    "98 0.0025234222 [(0.0025234222, 1.0002704)]\n",
    "99 0.0022875469 [(0.0022875469, 1.0002451)]\n",
    "100 0.0020739238 [(0.0020739238, 1.0002222)]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
