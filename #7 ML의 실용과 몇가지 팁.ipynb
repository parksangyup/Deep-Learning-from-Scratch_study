{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.15230925 \n",
      "Prediction:\n",
      " [[ 1.6346191 ]\n",
      " [ 0.06613699]\n",
      " [ 0.3500818 ]\n",
      " [ 0.67072517]\n",
      " [ 0.61130744]\n",
      " [ 0.61464405]\n",
      " [ 0.23171967]\n",
      " [-0.1372836 ]]\n",
      "1 Cost:  0.15230872 \n",
      "Prediction:\n",
      " [[ 1.634618  ]\n",
      " [ 0.06613836]\n",
      " [ 0.35008252]\n",
      " [ 0.670725  ]\n",
      " [ 0.6113076 ]\n",
      " [ 0.6146443 ]\n",
      " [ 0.23171999]\n",
      " [-0.13728246]]\n",
      "2 Cost:  0.15230817 \n",
      "Prediction:\n",
      " [[ 1.6346169 ]\n",
      " [ 0.06613982]\n",
      " [ 0.35008317]\n",
      " [ 0.67072475]\n",
      " [ 0.61130774]\n",
      " [ 0.6146444 ]\n",
      " [ 0.23172033]\n",
      " [-0.13728121]]\n",
      "3 Cost:  0.1523076 \n",
      "Prediction:\n",
      " [[ 1.6346157 ]\n",
      " [ 0.06614128]\n",
      " [ 0.3500839 ]\n",
      " [ 0.67072463]\n",
      " [ 0.6113078 ]\n",
      " [ 0.6146445 ]\n",
      " [ 0.23172063]\n",
      " [-0.13728008]]\n",
      "4 Cost:  0.15230703 \n",
      "Prediction:\n",
      " [[ 1.6346145 ]\n",
      " [ 0.06614277]\n",
      " [ 0.35008448]\n",
      " [ 0.67072445]\n",
      " [ 0.61130786]\n",
      " [ 0.6146447 ]\n",
      " [ 0.23172095]\n",
      " [-0.13727888]]\n",
      "5 Cost:  0.15230647 \n",
      "Prediction:\n",
      " [[ 1.6346133 ]\n",
      " [ 0.0661442 ]\n",
      " [ 0.35008517]\n",
      " [ 0.6707242 ]\n",
      " [ 0.611308  ]\n",
      " [ 0.6146449 ]\n",
      " [ 0.23172128]\n",
      " [-0.1372777 ]]\n",
      "6 Cost:  0.15230596 \n",
      "Prediction:\n",
      " [[ 1.6346122 ]\n",
      " [ 0.06614557]\n",
      " [ 0.3500858 ]\n",
      " [ 0.67072403]\n",
      " [ 0.611308  ]\n",
      " [ 0.6146451 ]\n",
      " [ 0.23172161]\n",
      " [-0.1372765 ]]\n",
      "7 Cost:  0.1523054 \n",
      "Prediction:\n",
      " [[ 1.634611  ]\n",
      " [ 0.06614703]\n",
      " [ 0.35008642]\n",
      " [ 0.67072386]\n",
      " [ 0.6113081 ]\n",
      " [ 0.61464524]\n",
      " [ 0.23172194]\n",
      " [-0.13727531]]\n",
      "8 Cost:  0.15230483 \n",
      "Prediction:\n",
      " [[ 1.6346098 ]\n",
      " [ 0.06614849]\n",
      " [ 0.3500871 ]\n",
      " [ 0.6707237 ]\n",
      " [ 0.6113082 ]\n",
      " [ 0.61464536]\n",
      " [ 0.23172225]\n",
      " [-0.13727418]]\n",
      "9 Cost:  0.15230429 \n",
      "Prediction:\n",
      " [[ 1.6346087 ]\n",
      " [ 0.06614998]\n",
      " [ 0.35008776]\n",
      " [ 0.67072344]\n",
      " [ 0.61130834]\n",
      " [ 0.6146456 ]\n",
      " [ 0.2317226 ]\n",
      " [-0.13727298]]\n",
      "10 Cost:  0.15230376 \n",
      "Prediction:\n",
      " [[ 1.6346076 ]\n",
      " [ 0.06615132]\n",
      " [ 0.35008848]\n",
      " [ 0.67072326]\n",
      " [ 0.61130846]\n",
      " [ 0.6146458 ]\n",
      " [ 0.23172289]\n",
      " [-0.13727179]]\n",
      "11 Cost:  0.15230317 \n",
      "Prediction:\n",
      " [[ 1.6346064 ]\n",
      " [ 0.06615278]\n",
      " [ 0.35008907]\n",
      " [ 0.670723  ]\n",
      " [ 0.61130846]\n",
      " [ 0.61464596]\n",
      " [ 0.2317232 ]\n",
      " [-0.1372706 ]]\n",
      "12 Cost:  0.15230262 \n",
      "Prediction:\n",
      " [[ 1.6346052 ]\n",
      " [ 0.06615427]\n",
      " [ 0.3500898 ]\n",
      " [ 0.67072284]\n",
      " [ 0.6113086 ]\n",
      " [ 0.6146461 ]\n",
      " [ 0.23172355]\n",
      " [-0.13726941]]\n",
      "13 Cost:  0.15230207 \n",
      "Prediction:\n",
      " [[ 1.634604  ]\n",
      " [ 0.0661557 ]\n",
      " [ 0.35009038]\n",
      " [ 0.67072266]\n",
      " [ 0.61130863]\n",
      " [ 0.6146462 ]\n",
      " [ 0.23172385]\n",
      " [-0.13726822]]\n",
      "14 Cost:  0.15230152 \n",
      "Prediction:\n",
      " [[ 1.6346029 ]\n",
      " [ 0.06615719]\n",
      " [ 0.3500911 ]\n",
      " [ 0.6707225 ]\n",
      " [ 0.61130875]\n",
      " [ 0.61464643]\n",
      " [ 0.23172417]\n",
      " [-0.13726708]]\n",
      "15 Cost:  0.15230095 \n",
      "Prediction:\n",
      " [[ 1.6346017 ]\n",
      " [ 0.06615853]\n",
      " [ 0.35009176]\n",
      " [ 0.67072225]\n",
      " [ 0.61130875]\n",
      " [ 0.61464655]\n",
      " [ 0.2317245 ]\n",
      " [-0.13726583]]\n",
      "16 Cost:  0.15230042 \n",
      "Prediction:\n",
      " [[ 1.6346005 ]\n",
      " [ 0.06615999]\n",
      " [ 0.35009238]\n",
      " [ 0.67072207]\n",
      " [ 0.6113089 ]\n",
      " [ 0.61464673]\n",
      " [ 0.2317248 ]\n",
      " [-0.1372647 ]]\n",
      "17 Cost:  0.15229988 \n",
      "Prediction:\n",
      " [[ 1.6345994 ]\n",
      " [ 0.06616148]\n",
      " [ 0.350093  ]\n",
      " [ 0.6707219 ]\n",
      " [ 0.61130905]\n",
      " [ 0.6146469 ]\n",
      " [ 0.23172516]\n",
      " [-0.1372635 ]]\n",
      "18 Cost:  0.15229934 \n",
      "Prediction:\n",
      " [[ 1.6345983 ]\n",
      " [ 0.06616279]\n",
      " [ 0.35009363]\n",
      " [ 0.6707217 ]\n",
      " [ 0.6113091 ]\n",
      " [ 0.61464703]\n",
      " [ 0.23172547]\n",
      " [-0.13726231]]\n",
      "19 Cost:  0.15229878 \n",
      "Prediction:\n",
      " [[ 1.6345971 ]\n",
      " [ 0.06616428]\n",
      " [ 0.35009432]\n",
      " [ 0.67072153]\n",
      " [ 0.6113092 ]\n",
      " [ 0.61464715]\n",
      " [ 0.23172581]\n",
      " [-0.13726112]]\n",
      "20 Cost:  0.15229821 \n",
      "Prediction:\n",
      " [[ 1.6345959 ]\n",
      " [ 0.06616575]\n",
      " [ 0.35009497]\n",
      " [ 0.6707213 ]\n",
      " [ 0.61130923]\n",
      " [ 0.6146474 ]\n",
      " [ 0.23172611]\n",
      " [-0.13725993]]\n",
      "21 Cost:  0.15229765 \n",
      "Prediction:\n",
      " [[ 1.6345947 ]\n",
      " [ 0.06616721]\n",
      " [ 0.3500957 ]\n",
      " [ 0.6707211 ]\n",
      " [ 0.6113093 ]\n",
      " [ 0.61464757]\n",
      " [ 0.23172642]\n",
      " [-0.1372588 ]]\n",
      "22 Cost:  0.15229711 \n",
      "Prediction:\n",
      " [[ 1.6345936 ]\n",
      " [ 0.0661687 ]\n",
      " [ 0.3500963 ]\n",
      " [ 0.67072093]\n",
      " [ 0.61130935]\n",
      " [ 0.61464775]\n",
      " [ 0.23172677]\n",
      " [-0.1372576 ]]\n",
      "23 Cost:  0.15229656 \n",
      "Prediction:\n",
      " [[ 1.6345924 ]\n",
      " [ 0.06617013]\n",
      " [ 0.350097  ]\n",
      " [ 0.67072076]\n",
      " [ 0.6113095 ]\n",
      " [ 0.61464787]\n",
      " [ 0.23172706]\n",
      " [-0.13725635]]\n",
      "24 Cost:  0.15229598 \n",
      "Prediction:\n",
      " [[ 1.6345912 ]\n",
      " [ 0.06617162]\n",
      " [ 0.3500976 ]\n",
      " [ 0.6707206 ]\n",
      " [ 0.61130965]\n",
      " [ 0.614648  ]\n",
      " [ 0.23172739]\n",
      " [-0.13725522]]\n",
      "25 Cost:  0.15229546 \n",
      "Prediction:\n",
      " [[ 1.6345901 ]\n",
      " [ 0.06617296]\n",
      " [ 0.35009828]\n",
      " [ 0.67072034]\n",
      " [ 0.6113097 ]\n",
      " [ 0.6146483 ]\n",
      " [ 0.23172772]\n",
      " [-0.13725403]]\n",
      "26 Cost:  0.1522949 \n",
      "Prediction:\n",
      " [[ 1.634589  ]\n",
      " [ 0.06617442]\n",
      " [ 0.3500989 ]\n",
      " [ 0.6707201 ]\n",
      " [ 0.61130977]\n",
      " [ 0.6146484 ]\n",
      " [ 0.23172802]\n",
      " [-0.13725284]]\n",
      "27 Cost:  0.15229434 \n",
      "Prediction:\n",
      " [[ 1.6345878 ]\n",
      " [ 0.06617591]\n",
      " [ 0.3500996 ]\n",
      " [ 0.67072   ]\n",
      " [ 0.6113098 ]\n",
      " [ 0.6146486 ]\n",
      " [ 0.23172837]\n",
      " [-0.13725165]]\n",
      "28 Cost:  0.15229379 \n",
      "Prediction:\n",
      " [[ 1.6345866 ]\n",
      " [ 0.06617725]\n",
      " [ 0.35010028]\n",
      " [ 0.67071974]\n",
      " [ 0.6113099 ]\n",
      " [ 0.6146487 ]\n",
      " [ 0.23172869]\n",
      " [-0.13725045]]\n",
      "29 Cost:  0.15229324 \n",
      "Prediction:\n",
      " [[ 1.6345854 ]\n",
      " [ 0.06617871]\n",
      " [ 0.3501009 ]\n",
      " [ 0.67071956]\n",
      " [ 0.61131   ]\n",
      " [ 0.6146489 ]\n",
      " [ 0.23172902]\n",
      " [-0.13724932]]\n",
      "30 Cost:  0.1522927 \n",
      "Prediction:\n",
      " [[ 1.6345843 ]\n",
      " [ 0.06618017]\n",
      " [ 0.35010153]\n",
      " [ 0.6707194 ]\n",
      " [ 0.61131006]\n",
      " [ 0.61464906]\n",
      " [ 0.23172933]\n",
      " [-0.13724813]]\n",
      "31 Cost:  0.15229213 \n",
      "Prediction:\n",
      " [[ 1.6345831 ]\n",
      " [ 0.06618163]\n",
      " [ 0.3501022 ]\n",
      " [ 0.67071915]\n",
      " [ 0.6113102 ]\n",
      " [ 0.61464924]\n",
      " [ 0.23172964]\n",
      " [-0.13724688]]\n",
      "32 Cost:  0.1522916 \n",
      "Prediction:\n",
      " [[ 1.6345819 ]\n",
      " [ 0.066183  ]\n",
      " [ 0.3501029 ]\n",
      " [ 0.67071897]\n",
      " [ 0.6113103 ]\n",
      " [ 0.61464936]\n",
      " [ 0.23172998]\n",
      " [-0.13724574]]\n",
      "33 Cost:  0.15229103 \n",
      "Prediction:\n",
      " [[ 1.6345809 ]\n",
      " [ 0.06618446]\n",
      " [ 0.35010356]\n",
      " [ 0.6707188 ]\n",
      " [ 0.61131036]\n",
      " [ 0.61464953]\n",
      " [ 0.23173028]\n",
      " [-0.13724455]]\n",
      "34 Cost:  0.15229046 \n",
      "Prediction:\n",
      " [[ 1.6345797 ]\n",
      " [ 0.06618592]\n",
      " [ 0.35010427]\n",
      " [ 0.6707186 ]\n",
      " [ 0.6113105 ]\n",
      " [ 0.61464965]\n",
      " [ 0.23173061]\n",
      " [-0.13724342]]\n",
      "35 Cost:  0.15228996 \n",
      "Prediction:\n",
      " [[ 1.6345785 ]\n",
      " [ 0.06618729]\n",
      " [ 0.3501048 ]\n",
      " [ 0.67071843]\n",
      " [ 0.61131054]\n",
      " [ 0.6146499 ]\n",
      " [ 0.23173094]\n",
      " [-0.13724223]]\n",
      "36 Cost:  0.15228939 \n",
      "Prediction:\n",
      " [[ 1.6345773 ]\n",
      " [ 0.06618872]\n",
      " [ 0.3501055 ]\n",
      " [ 0.6707182 ]\n",
      " [ 0.6113106 ]\n",
      " [ 0.6146501 ]\n",
      " [ 0.23173124]\n",
      " [-0.13724098]]\n",
      "37 Cost:  0.15228881 \n",
      "Prediction:\n",
      " [[ 1.6345761 ]\n",
      " [ 0.06619021]\n",
      " [ 0.35010612]\n",
      " [ 0.67071795]\n",
      " [ 0.6113107 ]\n",
      " [ 0.6146502 ]\n",
      " [ 0.2317316 ]\n",
      " [-0.13723984]]\n",
      "38 Cost:  0.15228829 \n",
      "Prediction:\n",
      " [[ 1.634575  ]\n",
      " [ 0.06619167]\n",
      " [ 0.3501068 ]\n",
      " [ 0.67071784]\n",
      " [ 0.6113108 ]\n",
      " [ 0.61465037]\n",
      " [ 0.2317319 ]\n",
      " [-0.13723865]]\n",
      "39 Cost:  0.15228772 \n",
      "Prediction:\n",
      " [[ 1.6345738 ]\n",
      " [ 0.06619313]\n",
      " [ 0.3501075 ]\n",
      " [ 0.67071766]\n",
      " [ 0.6113109 ]\n",
      " [ 0.6146505 ]\n",
      " [ 0.2317322 ]\n",
      " [-0.13723752]]\n",
      "40 Cost:  0.15228714 \n",
      "Prediction:\n",
      " [[ 1.6345726 ]\n",
      " [ 0.06619462]\n",
      " [ 0.35010815]\n",
      " [ 0.6707174 ]\n",
      " [ 0.611311  ]\n",
      " [ 0.61465067]\n",
      " [ 0.23173255]\n",
      " [-0.13723627]]\n",
      "41 Cost:  0.15228659 \n",
      "Prediction:\n",
      " [[ 1.6345716 ]\n",
      " [ 0.06619605]\n",
      " [ 0.35010886]\n",
      " [ 0.67071724]\n",
      " [ 0.6113111 ]\n",
      " [ 0.61465085]\n",
      " [ 0.23173286]\n",
      " [-0.13723508]]\n",
      "42 Cost:  0.15228608 \n",
      "Prediction:\n",
      " [[ 1.6345704 ]\n",
      " [ 0.06619743]\n",
      " [ 0.35010946]\n",
      " [ 0.670717  ]\n",
      " [ 0.61131114]\n",
      " [ 0.614651  ]\n",
      " [ 0.2317332 ]\n",
      " [-0.13723394]]\n",
      "43 Cost:  0.15228552 \n",
      "Prediction:\n",
      " [[ 1.6345692 ]\n",
      " [ 0.06619889]\n",
      " [ 0.3501101 ]\n",
      " [ 0.6707169 ]\n",
      " [ 0.6113112 ]\n",
      " [ 0.61465114]\n",
      " [ 0.2317335 ]\n",
      " [-0.13723275]]\n",
      "44 Cost:  0.15228495 \n",
      "Prediction:\n",
      " [[ 1.634568  ]\n",
      " [ 0.06620035]\n",
      " [ 0.3501107 ]\n",
      " [ 0.67071664]\n",
      " [ 0.6113113 ]\n",
      " [ 0.6146513 ]\n",
      " [ 0.23173383]\n",
      " [-0.1372315 ]]\n",
      "45 Cost:  0.15228441 \n",
      "Prediction:\n",
      " [[ 1.6345668 ]\n",
      " [ 0.06620172]\n",
      " [ 0.3501114 ]\n",
      " [ 0.67071646]\n",
      " [ 0.61131144]\n",
      " [ 0.61465156]\n",
      " [ 0.23173416]\n",
      " [-0.13723037]]\n",
      "46 Cost:  0.15228388 \n",
      "Prediction:\n",
      " [[ 1.6345657 ]\n",
      " [ 0.06620315]\n",
      " [ 0.35011208]\n",
      " [ 0.6707163 ]\n",
      " [ 0.61131144]\n",
      " [ 0.6146517 ]\n",
      " [ 0.23173445]\n",
      " [-0.13722917]]\n",
      "47 Cost:  0.1522833 \n",
      "Prediction:\n",
      " [[ 1.6345645 ]\n",
      " [ 0.06620464]\n",
      " [ 0.35011277]\n",
      " [ 0.67071605]\n",
      " [ 0.61131155]\n",
      " [ 0.61465186]\n",
      " [ 0.23173481]\n",
      " [-0.13722804]]\n",
      "48 Cost:  0.15228274 \n",
      "Prediction:\n",
      " [[ 1.6345633 ]\n",
      " [ 0.0662061 ]\n",
      " [ 0.3501134 ]\n",
      " [ 0.67071587]\n",
      " [ 0.6113117 ]\n",
      " [ 0.614652  ]\n",
      " [ 0.23173513]\n",
      " [-0.13722679]]\n",
      "49 Cost:  0.1522822 \n",
      "Prediction:\n",
      " [[ 1.6345623 ]\n",
      " [ 0.06620756]\n",
      " [ 0.35011408]\n",
      " [ 0.67071563]\n",
      " [ 0.61131173]\n",
      " [ 0.61465216]\n",
      " [ 0.23173542]\n",
      " [-0.1372256 ]]\n",
      "50 Cost:  0.15228164 \n",
      "Prediction:\n",
      " [[ 1.6345611 ]\n",
      " [ 0.06620893]\n",
      " [ 0.3501147 ]\n",
      " [ 0.6707155 ]\n",
      " [ 0.6113118 ]\n",
      " [ 0.61465234]\n",
      " [ 0.23173577]\n",
      " [-0.13722447]]\n",
      "51 Cost:  0.1522811 \n",
      "Prediction:\n",
      " [[ 1.6345599 ]\n",
      " [ 0.06621039]\n",
      " [ 0.35011536]\n",
      " [ 0.6707153 ]\n",
      " [ 0.6113119 ]\n",
      " [ 0.6146525 ]\n",
      " [ 0.23173608]\n",
      " [-0.13722327]]\n",
      "52 Cost:  0.15228057 \n",
      "Prediction:\n",
      " [[ 1.6345587 ]\n",
      " [ 0.06621173]\n",
      " [ 0.35011607]\n",
      " [ 0.6707151 ]\n",
      " [ 0.61131203]\n",
      " [ 0.6146527 ]\n",
      " [ 0.23173642]\n",
      " [-0.13722208]]\n",
      "53 Cost:  0.15228 \n",
      "Prediction:\n",
      " [[ 1.6345575 ]\n",
      " [ 0.06621319]\n",
      " [ 0.35011667]\n",
      " [ 0.67071486]\n",
      " [ 0.6113121 ]\n",
      " [ 0.6146528 ]\n",
      " [ 0.23173672]\n",
      " [-0.13722089]]\n",
      "54 Cost:  0.15227945 \n",
      "Prediction:\n",
      " [[ 1.6345564 ]\n",
      " [ 0.06621465]\n",
      " [ 0.35011733]\n",
      " [ 0.67071474]\n",
      " [ 0.6113122 ]\n",
      " [ 0.614653  ]\n",
      " [ 0.23173705]\n",
      " [-0.1372197 ]]\n",
      "55 Cost:  0.1522789 \n",
      "Prediction:\n",
      " [[ 1.6345552 ]\n",
      " [ 0.06621614]\n",
      " [ 0.35011792]\n",
      " [ 0.6707145 ]\n",
      " [ 0.61131227]\n",
      " [ 0.61465317]\n",
      " [ 0.23173738]\n",
      " [-0.13721856]]\n",
      "56 Cost:  0.15227833 \n",
      "Prediction:\n",
      " [[ 1.634554  ]\n",
      " [ 0.0662176 ]\n",
      " [ 0.3501186 ]\n",
      " [ 0.6707143 ]\n",
      " [ 0.6113124 ]\n",
      " [ 0.61465335]\n",
      " [ 0.23173767]\n",
      " [-0.13721737]]\n",
      "57 Cost:  0.15227777 \n",
      "Prediction:\n",
      " [[ 1.634553  ]\n",
      " [ 0.06621906]\n",
      " [ 0.3501193 ]\n",
      " [ 0.67071414]\n",
      " [ 0.61131245]\n",
      " [ 0.61465347]\n",
      " [ 0.23173803]\n",
      " [-0.13721612]]\n",
      "58 Cost:  0.1522772 \n",
      "Prediction:\n",
      " [[ 1.6345518 ]\n",
      " [ 0.06622055]\n",
      " [ 0.35011998]\n",
      " [ 0.67071396]\n",
      " [ 0.6113125 ]\n",
      " [ 0.61465365]\n",
      " [ 0.23173834]\n",
      " [-0.13721499]]\n",
      "59 Cost:  0.15227668 \n",
      "Prediction:\n",
      " [[ 1.6345506 ]\n",
      " [ 0.06622186]\n",
      " [ 0.35012066]\n",
      " [ 0.6707137 ]\n",
      " [ 0.6113126 ]\n",
      " [ 0.6146538 ]\n",
      " [ 0.23173864]\n",
      " [-0.1372138 ]]\n",
      "60 Cost:  0.1522761 \n",
      "Prediction:\n",
      " [[ 1.6345494 ]\n",
      " [ 0.06622335]\n",
      " [ 0.3501213 ]\n",
      " [ 0.67071354]\n",
      " [ 0.6113127 ]\n",
      " [ 0.61465406]\n",
      " [ 0.23173898]\n",
      " [-0.13721266]]\n",
      "61 Cost:  0.15227555 \n",
      "Prediction:\n",
      " [[ 1.6345482 ]\n",
      " [ 0.06622481]\n",
      " [ 0.35012192]\n",
      " [ 0.67071337]\n",
      " [ 0.61131275]\n",
      " [ 0.6146542 ]\n",
      " [ 0.2317393 ]\n",
      " [-0.13721141]]\n",
      "62 Cost:  0.15227504 \n",
      "Prediction:\n",
      " [[ 1.6345471 ]\n",
      " [ 0.06622615]\n",
      " [ 0.3501225 ]\n",
      " [ 0.6707131 ]\n",
      " [ 0.61131287]\n",
      " [ 0.6146543 ]\n",
      " [ 0.23173961]\n",
      " [-0.13721022]]\n",
      "63 Cost:  0.15227446 \n",
      "Prediction:\n",
      " [[ 1.6345459 ]\n",
      " [ 0.06622764]\n",
      " [ 0.35012323]\n",
      " [ 0.67071295]\n",
      " [ 0.611313  ]\n",
      " [ 0.6146544 ]\n",
      " [ 0.23173994]\n",
      " [-0.13720909]]\n",
      "64 Cost:  0.15227391 \n",
      "Prediction:\n",
      " [[ 1.6345447 ]\n",
      " [ 0.06622908]\n",
      " [ 0.35012388]\n",
      " [ 0.6707127 ]\n",
      " [ 0.61131305]\n",
      " [ 0.61465466]\n",
      " [ 0.23174027]\n",
      " [-0.1372079 ]]\n",
      "65 Cost:  0.15227337 \n",
      "Prediction:\n",
      " [[ 1.6345437 ]\n",
      " [ 0.06623057]\n",
      " [ 0.3501246 ]\n",
      " [ 0.6707126 ]\n",
      " [ 0.6113131 ]\n",
      " [ 0.6146548 ]\n",
      " [ 0.2317406 ]\n",
      " [-0.1372067 ]]\n",
      "66 Cost:  0.1522728 \n",
      "Prediction:\n",
      " [[ 1.6345425 ]\n",
      " [ 0.06623203]\n",
      " [ 0.3501252 ]\n",
      " [ 0.67071235]\n",
      " [ 0.61131316]\n",
      " [ 0.614655  ]\n",
      " [ 0.23174089]\n",
      " [-0.13720551]]\n",
      "67 Cost:  0.15227225 \n",
      "Prediction:\n",
      " [[ 1.6345413 ]\n",
      " [ 0.06623337]\n",
      " [ 0.35012588]\n",
      " [ 0.6707122 ]\n",
      " [ 0.6113133 ]\n",
      " [ 0.61465514]\n",
      " [ 0.23174125]\n",
      " [-0.13720432]]\n",
      "68 Cost:  0.15227172 \n",
      "Prediction:\n",
      " [[ 1.6345401 ]\n",
      " [ 0.06623486]\n",
      " [ 0.3501265 ]\n",
      " [ 0.670712  ]\n",
      " [ 0.61131346]\n",
      " [ 0.61465526]\n",
      " [ 0.23174155]\n",
      " [-0.13720319]]\n",
      "69 Cost:  0.15227115 \n",
      "Prediction:\n",
      " [[ 1.6345389 ]\n",
      " [ 0.06623629]\n",
      " [ 0.3501272 ]\n",
      " [ 0.6707118 ]\n",
      " [ 0.61131346]\n",
      " [ 0.61465544]\n",
      " [ 0.23174186]\n",
      " [-0.13720194]]\n",
      "70 Cost:  0.15227062 \n",
      "Prediction:\n",
      " [[ 1.6345378 ]\n",
      " [ 0.06623766]\n",
      " [ 0.35012788]\n",
      " [ 0.67071164]\n",
      " [ 0.6113136 ]\n",
      " [ 0.6146556 ]\n",
      " [ 0.2317422 ]\n",
      " [-0.1372008 ]]\n",
      "71 Cost:  0.15227006 \n",
      "Prediction:\n",
      " [[ 1.6345366 ]\n",
      " [ 0.06623912]\n",
      " [ 0.35012853]\n",
      " [ 0.6707114 ]\n",
      " [ 0.61131364]\n",
      " [ 0.61465585]\n",
      " [ 0.23174252]\n",
      " [-0.13719961]]\n",
      "72 Cost:  0.15226951 \n",
      "Prediction:\n",
      " [[ 1.6345354 ]\n",
      " [ 0.06624058]\n",
      " [ 0.35012913]\n",
      " [ 0.6707112 ]\n",
      " [ 0.6113137 ]\n",
      " [ 0.614656  ]\n",
      " [ 0.23174283]\n",
      " [-0.13719842]]\n",
      "73 Cost:  0.15226895 \n",
      "Prediction:\n",
      " [[ 1.6345344 ]\n",
      " [ 0.06624207]\n",
      " [ 0.35012978]\n",
      " [ 0.67071104]\n",
      " [ 0.6113138 ]\n",
      " [ 0.6146561 ]\n",
      " [ 0.23174316]\n",
      " [-0.13719723]]\n",
      "74 Cost:  0.15226838 \n",
      "Prediction:\n",
      " [[ 1.6345332 ]\n",
      " [ 0.06624353]\n",
      " [ 0.35013044]\n",
      " [ 0.6707108 ]\n",
      " [ 0.61131394]\n",
      " [ 0.6146563 ]\n",
      " [ 0.23174348]\n",
      " [-0.13719603]]\n",
      "75 Cost:  0.15226784 \n",
      "Prediction:\n",
      " [[ 1.634532  ]\n",
      " [ 0.06624499]\n",
      " [ 0.3501311 ]\n",
      " [ 0.6707107 ]\n",
      " [ 0.611314  ]\n",
      " [ 0.6146565 ]\n",
      " [ 0.23174381]\n",
      " [-0.1371949 ]]\n",
      "76 Cost:  0.1522673 \n",
      "Prediction:\n",
      " [[ 1.6345308 ]\n",
      " [ 0.06624633]\n",
      " [ 0.35013178]\n",
      " [ 0.67071044]\n",
      " [ 0.6113141 ]\n",
      " [ 0.6146567 ]\n",
      " [ 0.23174411]\n",
      " [-0.13719371]]\n",
      "77 Cost:  0.15226671 \n",
      "Prediction:\n",
      " [[ 1.6345296 ]\n",
      " [ 0.06624779]\n",
      " [ 0.35013247]\n",
      " [ 0.6707102 ]\n",
      " [ 0.6113142 ]\n",
      " [ 0.6146568 ]\n",
      " [ 0.23174447]\n",
      " [-0.13719246]]\n",
      "78 Cost:  0.15226619 \n",
      "Prediction:\n",
      " [[ 1.6345285 ]\n",
      " [ 0.06624928]\n",
      " [ 0.35013315]\n",
      " [ 0.67071   ]\n",
      " [ 0.61131424]\n",
      " [ 0.6146569 ]\n",
      " [ 0.23174477]\n",
      " [-0.13719133]]\n",
      "79 Cost:  0.15226565 \n",
      "Prediction:\n",
      " [[ 1.6345273 ]\n",
      " [ 0.06625062]\n",
      " [ 0.35013378]\n",
      " [ 0.67070985]\n",
      " [ 0.61131436]\n",
      " [ 0.61465716]\n",
      " [ 0.23174508]\n",
      " [-0.13719013]]\n",
      "80 Cost:  0.15226507 \n",
      "Prediction:\n",
      " [[ 1.6345261 ]\n",
      " [ 0.06625208]\n",
      " [ 0.35013446]\n",
      " [ 0.6707096 ]\n",
      " [ 0.6113144 ]\n",
      " [ 0.6146573 ]\n",
      " [ 0.23174542]\n",
      " [-0.13718894]]\n",
      "81 Cost:  0.15226455 \n",
      "Prediction:\n",
      " [[ 1.6345251 ]\n",
      " [ 0.06625357]\n",
      " [ 0.35013503]\n",
      " [ 0.6707095 ]\n",
      " [ 0.6113145 ]\n",
      " [ 0.6146575 ]\n",
      " [ 0.23174573]\n",
      " [-0.13718781]]\n",
      "82 Cost:  0.152264 \n",
      "Prediction:\n",
      " [[ 1.6345239 ]\n",
      " [ 0.066255  ]\n",
      " [ 0.35013568]\n",
      " [ 0.67070925]\n",
      " [ 0.6113146 ]\n",
      " [ 0.61465764]\n",
      " [ 0.23174605]\n",
      " [-0.13718656]]\n",
      "83 Cost:  0.1522634 \n",
      "Prediction:\n",
      " [[ 1.6345227 ]\n",
      " [ 0.06625649]\n",
      " [ 0.3501364 ]\n",
      " [ 0.6707091 ]\n",
      " [ 0.6113147 ]\n",
      " [ 0.61465776]\n",
      " [ 0.23174638]\n",
      " [-0.13718542]]\n",
      "84 Cost:  0.15226288 \n",
      "Prediction:\n",
      " [[ 1.6345215 ]\n",
      " [ 0.06625783]\n",
      " [ 0.35013705]\n",
      " [ 0.6707089 ]\n",
      " [ 0.6113148 ]\n",
      " [ 0.61465794]\n",
      " [ 0.2317467 ]\n",
      " [-0.13718423]]\n",
      "85 Cost:  0.15226233 \n",
      "Prediction:\n",
      " [[ 1.6345203 ]\n",
      " [ 0.06625929]\n",
      " [ 0.3501377 ]\n",
      " [ 0.6707087 ]\n",
      " [ 0.61131483]\n",
      " [ 0.6146581 ]\n",
      " [ 0.231747  ]\n",
      " [-0.13718304]]\n",
      "86 Cost:  0.15226176 \n",
      "Prediction:\n",
      " [[ 1.6345192 ]\n",
      " [ 0.06626078]\n",
      " [ 0.35013837]\n",
      " [ 0.6707085 ]\n",
      " [ 0.61131495]\n",
      " [ 0.6146583 ]\n",
      " [ 0.23174733]\n",
      " [-0.13718185]]\n",
      "87 Cost:  0.15226123 \n",
      "Prediction:\n",
      " [[ 1.634518  ]\n",
      " [ 0.0662621 ]\n",
      " [ 0.350139  ]\n",
      " [ 0.6707083 ]\n",
      " [ 0.611315  ]\n",
      " [ 0.6146584 ]\n",
      " [ 0.23174769]\n",
      " [-0.13718066]]\n",
      "88 Cost:  0.15226066 \n",
      "Prediction:\n",
      " [[ 1.6345168 ]\n",
      " [ 0.06626371]\n",
      " [ 0.35013968]\n",
      " [ 0.6707081 ]\n",
      " [ 0.6113151 ]\n",
      " [ 0.6146586 ]\n",
      " [ 0.23174798]\n",
      " [-0.13717952]]\n",
      "89 Cost:  0.15226012 \n",
      "Prediction:\n",
      " [[ 1.6345158 ]\n",
      " [ 0.06626505]\n",
      " [ 0.35014036]\n",
      " [ 0.67070794]\n",
      " [ 0.6113152 ]\n",
      " [ 0.6146587 ]\n",
      " [ 0.2317483 ]\n",
      " [-0.13717833]]\n",
      "90 Cost:  0.15225956 \n",
      "Prediction:\n",
      " [[ 1.6345146 ]\n",
      " [ 0.06626651]\n",
      " [ 0.35014093]\n",
      " [ 0.6707077 ]\n",
      " [ 0.61131525]\n",
      " [ 0.61465895]\n",
      " [ 0.23174864]\n",
      " [-0.13717708]]\n",
      "91 Cost:  0.152259 \n",
      "Prediction:\n",
      " [[ 1.6345134 ]\n",
      " [ 0.066268  ]\n",
      " [ 0.3501416 ]\n",
      " [ 0.6707075 ]\n",
      " [ 0.61131537]\n",
      " [ 0.61465913]\n",
      " [ 0.23174895]\n",
      " [-0.13717595]]\n",
      "92 Cost:  0.15225846 \n",
      "Prediction:\n",
      " [[ 1.6345122 ]\n",
      " [ 0.06626943]\n",
      " [ 0.35014224]\n",
      " [ 0.67070735]\n",
      " [ 0.6113155 ]\n",
      " [ 0.61465925]\n",
      " [ 0.23174927]\n",
      " [-0.13717476]]\n",
      "93 Cost:  0.15225787 \n",
      "Prediction:\n",
      " [[ 1.634511  ]\n",
      " [ 0.06627092]\n",
      " [ 0.3501429 ]\n",
      " [ 0.6707071 ]\n",
      " [ 0.61131555]\n",
      " [ 0.6146594 ]\n",
      " [ 0.2317496 ]\n",
      " [-0.13717356]]\n",
      "94 Cost:  0.15225735 \n",
      "Prediction:\n",
      " [[ 1.6345099 ]\n",
      " [ 0.06627226]\n",
      " [ 0.3501436 ]\n",
      " [ 0.6707069 ]\n",
      " [ 0.6113156 ]\n",
      " [ 0.6146596 ]\n",
      " [ 0.23174992]\n",
      " [-0.13717237]]\n",
      "95 Cost:  0.1522568 \n",
      "Prediction:\n",
      " [[ 1.6345087 ]\n",
      " [ 0.06627372]\n",
      " [ 0.35014427]\n",
      " [ 0.67070675]\n",
      " [ 0.6113157 ]\n",
      " [ 0.6146598 ]\n",
      " [ 0.23175022]\n",
      " [-0.13717118]]\n",
      "96 Cost:  0.15225622 \n",
      "Prediction:\n",
      " [[ 1.6345075 ]\n",
      " [ 0.06627521]\n",
      " [ 0.35014498]\n",
      " [ 0.6707065 ]\n",
      " [ 0.6113157 ]\n",
      " [ 0.61465997]\n",
      " [ 0.23175055]\n",
      " [-0.13717005]]\n",
      "97 Cost:  0.15225571 \n",
      "Prediction:\n",
      " [[ 1.6345065 ]\n",
      " [ 0.06627655]\n",
      " [ 0.35014558]\n",
      " [ 0.67070633]\n",
      " [ 0.6113159 ]\n",
      " [ 0.6146601 ]\n",
      " [ 0.2317509 ]\n",
      " [-0.13716885]]\n",
      "98 Cost:  0.15225515 \n",
      "Prediction:\n",
      " [[ 1.6345053 ]\n",
      " [ 0.06627801]\n",
      " [ 0.3501462 ]\n",
      " [ 0.67070615]\n",
      " [ 0.611316  ]\n",
      " [ 0.61466026]\n",
      " [ 0.2317512 ]\n",
      " [-0.1371676 ]]\n",
      "99 Cost:  0.15225461 \n",
      "Prediction:\n",
      " [[ 1.6345041 ]\n",
      " [ 0.06627947]\n",
      " [ 0.35014683]\n",
      " [ 0.670706  ]\n",
      " [ 0.6113161 ]\n",
      " [ 0.61466044]\n",
      " [ 0.23175152]\n",
      " [-0.13716647]]\n",
      "100 Cost:  0.15225402 \n",
      "Prediction:\n",
      " [[ 1.6345029 ]\n",
      " [ 0.06628093]\n",
      " [ 0.35014752]\n",
      " [ 0.67070574]\n",
      " [ 0.61131614]\n",
      " [ 0.6146606 ]\n",
      " [ 0.23175186]\n",
      " [-0.13716528]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Cost: 0.15230925 \\nPrediction:\\n [[ 1.6346191 ]\\n [ 0.06613699]\\n [ 0.3500818 ]\\n [ 0.6707252 ]\\n [ 0.61130744]\\n [ 0.61464405]\\n [ 0.23171967]\\n [-0.1372836 ]]\\n1 Cost: 0.15230872 \\nPrediction:\\n [[ 1.634618  ]\\n [ 0.06613836]\\n [ 0.35008252]\\n [ 0.670725  ]\\n [ 0.6113076 ]\\n [ 0.6146443 ]\\n [ 0.23172   ]\\n [-0.13728246]]\\n...\\n99 Cost: 0.1522546 \\nPrediction:\\n [[ 1.6345041 ]\\n [ 0.06627947]\\n [ 0.35014683]\\n [ 0.670706  ]\\n [ 0.6113161 ]\\n [ 0.61466044]\\n [ 0.23175153]\\n [-0.13716647]]\\n100 Cost: 0.15225402 \\nPrediction:\\n [[ 1.6345029 ]\\n [ 0.06628093]\\n [ 0.35014752]\\n [ 0.67070574]\\n [ 0.61131614]\\n [ 0.6146606 ]\\n [ 0.23175186]\\n [-0.13716528]]\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#이제부턴 학습과 테스트셋을 나눠서 학습을 시킴 !! \n",
    "#반드시test데이터와 train데이터를 나눠야한다. \n",
    "#학습이 끝난다음에 test데이터를 통해 모델을 테스트한다.\n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
    "\n",
    "'''\n",
    "when lr = 1.5\n",
    "0 5.73203 [[-0.30548954  1.22985029 -0.66033536]\n",
    " [-4.39069986  2.29670858  2.99386835]\n",
    " [-3.34510708  2.09743214 -0.80419564]]\n",
    "1 23.1494 [[ 0.06951046  0.29449689 -0.0999819 ]\n",
    " [-1.95319986 -1.63627958  4.48935604]\n",
    " [-0.90760708 -1.65020132  0.50593793]]\n",
    "2 27.2798 [[ 0.44451016  0.85699677 -1.03748143]\n",
    " [ 0.48429942  0.98872018 -0.57314301]\n",
    " [ 1.52989244  1.16229868 -4.74406147]]\n",
    "3 8.668 [[ 0.12396193  0.61504567 -0.47498202]\n",
    " [ 0.22003263 -0.2470119   0.9268558 ]\n",
    " [ 0.96035379  0.41933775 -3.43156195]]\n",
    "4 5.77111 [[-0.9524312   1.13037777  0.08607888]\n",
    " [-3.78651619  2.26245379  2.42393875]\n",
    " [-3.07170963  3.14037919 -2.12054014]]\n",
    "5 inf [[ nan  nan  nan]\n",
    " [ nan  nan  nan]\n",
    " [ nan  nan  nan]]\n",
    "6 nan [[ nan  nan  nan]\n",
    " [ nan  nan  nan]\n",
    " [ nan  nan  nan]]\n",
    " ...\n",
    "Prediction: [0 0 0]\n",
    "Accuracy:  0.0\n",
    "-------------------------------------------------\n",
    "When lr = 1e-10\n",
    "0 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
    " [-0.3051686  -0.3032113   1.50825703]\n",
    " [ 0.75722361 -0.7008909  -2.10820389]]\n",
    "1 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
    " [-0.3051686  -0.3032113   1.50825703]\n",
    " [ 0.75722361 -0.7008909  -2.10820389]]\n",
    "...\n",
    "199 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
    " [-0.3051686  -0.3032113   1.50825703]\n",
    " [ 0.75722361 -0.7008909  -2.10820389]]\n",
    "200 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
    " [-0.3051686  -0.3032113   1.50825703]\n",
    " [ 0.75722361 -0.7008909  -2.10820389]]\n",
    "Prediction: [0 0 0]\n",
    "Accuracy:  0.0\n",
    "-------------------------------------------------\n",
    "When lr = 0.1\n",
    "0 5.73203 [[ 0.72881663  0.71536207 -1.18015325]\n",
    " [-0.57753736 -0.12988332  1.60729778]\n",
    " [ 0.48373488 -0.51433605 -2.02127004]]\n",
    "1 3.318 [[ 0.66219079  0.74796319 -1.14612854]\n",
    " [-0.81948912  0.03000021  1.68936598]\n",
    " [ 0.23214608 -0.33772916 -1.94628811]]\n",
    "...\n",
    "199 0.672261 [[-1.15377033  0.28146935  1.13632679]\n",
    " [ 0.37484586  0.18958236  0.33544877]\n",
    " [-0.35609841 -0.43973011 -1.25604188]]\n",
    "200 0.670909 [[-1.15885413  0.28058422  1.14229572]\n",
    " [ 0.37609792  0.19073224  0.33304682]\n",
    " [-0.35536593 -0.44033223 -1.2561723 ]]\n",
    "Prediction: [2 2 2]\n",
    "Accuracy:  1.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  120882725000.0 \n",
      "Prediction:\n",
      " [[246290.98]\n",
      " [494301.56]\n",
      " [389156.4 ]\n",
      " [273227.28]\n",
      " [321757.5 ]\n",
      " [324450.72]\n",
      " [297479.62]\n",
      " [378352.44]]\n",
      "1 Cost:  1.328115e+26 \n",
      "Prediction:\n",
      " [[-8.1292254e+12]\n",
      " [-1.6364964e+13]\n",
      " [-1.2873726e+13]\n",
      " [-9.0244142e+12]\n",
      " [-1.0635755e+13]\n",
      " [-1.0725273e+13]\n",
      " [-9.8300844e+12]\n",
      " [-1.2515651e+13]]\n",
      "2 Cost:  inf \n",
      "Prediction:\n",
      " [[2.6945415e+20]\n",
      " [5.4243884e+20]\n",
      " [4.2671705e+20]\n",
      " [2.9912638e+20]\n",
      " [3.5253643e+20]\n",
      " [3.5550367e+20]\n",
      " [3.2583142e+20]\n",
      " [4.1484816e+20]]\n",
      "3 Cost:  inf \n",
      "Prediction:\n",
      " [[-8.9314212e+27]\n",
      " [-1.7979867e+28]\n",
      " [-1.4144113e+28]\n",
      " [-9.9149473e+27]\n",
      " [-1.1685296e+28]\n",
      " [-1.1783649e+28]\n",
      " [-1.0800121e+28]\n",
      " [-1.3750701e+28]]\n",
      "4 Cost:  inf \n",
      "Prediction:\n",
      " [[2.9604401e+35]\n",
      " [5.9596695e+35]\n",
      " [4.6882568e+35]\n",
      " [3.2864432e+35]\n",
      " [3.8732490e+35]\n",
      " [3.9058490e+35]\n",
      " [3.5798461e+35]\n",
      " [4.5578556e+35]]\n",
      "5 Cost:  inf \n",
      "Prediction:\n",
      " [[-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]\n",
      " [-inf]]\n",
      "6 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "7 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "8 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "9 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "10 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "11 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "12 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "13 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "14 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "15 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "16 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "17 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "18 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "19 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "20 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "21 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "22 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "23 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "24 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "25 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "26 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "27 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "28 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "29 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "30 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "31 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "32 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "33 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "34 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "35 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "36 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "37 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "38 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "39 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "40 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "41 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "42 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "43 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "44 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "45 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "46 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "47 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "48 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "49 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "50 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "51 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "52 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "53 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "54 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "55 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "56 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "57 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "58 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "59 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "60 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "61 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "62 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "63 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "64 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "65 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "66 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "67 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "68 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "69 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "70 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "71 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "72 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "73 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "74 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "75 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "76 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "77 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "78 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "79 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "80 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "81 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "82 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "83 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "84 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "85 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "86 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "87 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "88 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "89 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "90 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "91 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "92 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "93 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "94 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "95 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "96 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "97 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "98 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "99 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n",
      "100 Cost:  nan \n",
      "Prediction:\n",
      " [[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Cost:  2.45533e+12\\nPrediction:\\n [[-1104436.375]\\n [-2224342.75 ]\\n [-1749606.75 ]\\n [-1226179.375]\\n [-1445287.125]\\n [-1457459.5  ]\\n [-1335740.5  ]\\n [-1700924.625]]\\n1 Cost:  2.69762e+27\\nPrediction:\\n [[  3.66371490e+13]\\n [  7.37543360e+13]\\n [  5.80198785e+13]\\n [  4.06716290e+13]\\n [  4.79336847e+13]\\n [  4.83371348e+13]\\n [  4.43026590e+13]\\n [  5.64060907e+13]]\\n2 Cost:  inf\\nPrediction:\\n [[ -1.21438790e+21]\\n [ -2.44468702e+21]\\n [ -1.92314724e+21]\\n [ -1.34811610e+21]\\n [ -1.58882674e+21]\\n [ -1.60219962e+21]\\n [ -1.46847142e+21]\\n [ -1.86965602e+21]]\\n3 Cost:  inf\\nPrediction:\\n [[  4.02525216e+28]\\n [  8.10324465e+28]\\n [  6.37453079e+28]\\n [  4.46851237e+28]\\n [  5.26638074e+28]\\n [  5.31070676e+28]\\n [  4.86744608e+28]\\n [  6.19722623e+28]]\\n4 Cost:  inf\\nPrediction:\\n [[ -1.33422428e+36]\\n [ -2.68593010e+36]\\n [ -2.11292430e+36]\\n [ -1.48114879e+36]\\n [ -1.74561303e+36]\\n [ -1.76030542e+36]\\n [ -1.61338091e+36]\\n [ -2.05415459e+36]]\\n5 Cost:  inf\\nPrediction:\\n [[ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]\\n [ inf]]\\n6 Cost:  nan\\nPrediction:\\n [[ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]\\n [ nan]]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "\n",
    "'''\n",
    "0 Cost:  2.45533e+12\n",
    "Prediction:\n",
    " [[-1104436.375]\n",
    " [-2224342.75 ]\n",
    " [-1749606.75 ]\n",
    " [-1226179.375]\n",
    " [-1445287.125]\n",
    " [-1457459.5  ]\n",
    " [-1335740.5  ]\n",
    " [-1700924.625]]\n",
    "1 Cost:  2.69762e+27\n",
    "Prediction:\n",
    " [[  3.66371490e+13]\n",
    " [  7.37543360e+13]\n",
    " [  5.80198785e+13]\n",
    " [  4.06716290e+13]\n",
    " [  4.79336847e+13]\n",
    " [  4.83371348e+13]\n",
    " [  4.43026590e+13]\n",
    " [  5.64060907e+13]]\n",
    "2 Cost:  inf\n",
    "Prediction:\n",
    " [[ -1.21438790e+21]\n",
    " [ -2.44468702e+21]\n",
    " [ -1.92314724e+21]\n",
    " [ -1.34811610e+21]\n",
    " [ -1.58882674e+21]\n",
    " [ -1.60219962e+21]\n",
    " [ -1.46847142e+21]\n",
    " [ -1.86965602e+21]]\n",
    "3 Cost:  inf\n",
    "Prediction:\n",
    " [[  4.02525216e+28]\n",
    " [  8.10324465e+28]\n",
    " [  6.37453079e+28]\n",
    " [  4.46851237e+28]\n",
    " [  5.26638074e+28]\n",
    " [  5.31070676e+28]\n",
    " [  4.86744608e+28]\n",
    " [  6.19722623e+28]]\n",
    "4 Cost:  inf\n",
    "Prediction:\n",
    " [[ -1.33422428e+36]\n",
    " [ -2.68593010e+36]\n",
    " [ -2.11292430e+36]\n",
    " [ -1.48114879e+36]\n",
    " [ -1.74561303e+36]\n",
    " [ -1.76030542e+36]\n",
    " [ -1.61338091e+36]\n",
    " [ -2.05415459e+36]]\n",
    "5 Cost:  inf\n",
    "Prediction:##여기서 무한대로 발산하기 시작함\n",
    " [[ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]\n",
    " [ inf]]\n",
    "6 Cost:  nan\n",
    "Prediction:##이러면 학습을 포기하기 시작함 이런경우엔 learning rate가 너무 큰것이 아닌가 의심해봐야하는것\n",
    " [[ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]\n",
    " [ nan]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
      " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
      " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
      " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
      " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
      " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
      " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
      " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
      "0 Cost:  0.56470245 \n",
      "Prediction:\n",
      " [[-0.06572413]\n",
      " [-0.06795955]\n",
      " [-0.13650313]\n",
      " [-0.2401303 ]\n",
      " [-0.16231954]\n",
      " [-0.2708047 ]\n",
      " [-0.3281756 ]\n",
      " [-0.5223048 ]]\n",
      "1 Cost:  0.5646596 \n",
      "Prediction:\n",
      " [[-0.06568605]\n",
      " [-0.06792274]\n",
      " [-0.13647231]\n",
      " [-0.24010634]\n",
      " [-0.16229117]\n",
      " [-0.2707773 ]\n",
      " [-0.32815713]\n",
      " [-0.5222866 ]]\n",
      "2 Cost:  0.5646168 \n",
      "Prediction:\n",
      " [[-0.06564796]\n",
      " [-0.06788588]\n",
      " [-0.13644147]\n",
      " [-0.24008232]\n",
      " [-0.16226292]\n",
      " [-0.27075   ]\n",
      " [-0.32813865]\n",
      " [-0.5222684 ]]\n",
      "3 Cost:  0.564574 \n",
      "Prediction:\n",
      " [[-0.06560993]\n",
      " [-0.06784898]\n",
      " [-0.13641065]\n",
      " [-0.24005836]\n",
      " [-0.1622346 ]\n",
      " [-0.27072263]\n",
      " [-0.32812017]\n",
      " [-0.5222503 ]]\n",
      "4 Cost:  0.5645312 \n",
      "Prediction:\n",
      " [[-0.0655719 ]\n",
      " [-0.06781217]\n",
      " [-0.13637993]\n",
      " [-0.24003434]\n",
      " [-0.16220629]\n",
      " [-0.2706952 ]\n",
      " [-0.32810166]\n",
      " [-0.5222321 ]]\n",
      "5 Cost:  0.5644884 \n",
      "Prediction:\n",
      " [[-0.06553382]\n",
      " [-0.06777528]\n",
      " [-0.13634908]\n",
      " [-0.24001038]\n",
      " [-0.16217798]\n",
      " [-0.27066788]\n",
      " [-0.32808316]\n",
      " [-0.522214  ]]\n",
      "6 Cost:  0.56444556 \n",
      "Prediction:\n",
      " [[-0.06549573]\n",
      " [-0.06773847]\n",
      " [-0.1363183 ]\n",
      " [-0.23998642]\n",
      " [-0.16214967]\n",
      " [-0.27064046]\n",
      " [-0.32806468]\n",
      " [-0.5221958 ]]\n",
      "7 Cost:  0.56440276 \n",
      "Prediction:\n",
      " [[-0.0654577 ]\n",
      " [-0.06770158]\n",
      " [-0.13628748]\n",
      " [-0.2399624 ]\n",
      " [-0.16212133]\n",
      " [-0.27061313]\n",
      " [-0.3280462 ]\n",
      " [-0.5221777 ]]\n",
      "8 Cost:  0.5643599 \n",
      "Prediction:\n",
      " [[-0.06541967]\n",
      " [-0.06766474]\n",
      " [-0.13625664]\n",
      " [-0.23993844]\n",
      " [-0.16209301]\n",
      " [-0.27058578]\n",
      " [-0.32802773]\n",
      " [-0.5221595 ]]\n",
      "9 Cost:  0.5643171 \n",
      "Prediction:\n",
      " [[-0.06538159]\n",
      " [-0.06762791]\n",
      " [-0.13622582]\n",
      " [-0.23991442]\n",
      " [-0.1620647 ]\n",
      " [-0.2705584 ]\n",
      " [-0.32800925]\n",
      " [-0.52214134]]\n",
      "10 Cost:  0.56427425 \n",
      "Prediction:\n",
      " [[-0.0653435 ]\n",
      " [-0.06759101]\n",
      " [-0.136195  ]\n",
      " [-0.23989046]\n",
      " [-0.16203636]\n",
      " [-0.27053106]\n",
      " [-0.32799074]\n",
      " [-0.5221232 ]]\n",
      "11 Cost:  0.5642314 \n",
      "Prediction:\n",
      " [[-0.06530547]\n",
      " [-0.06755415]\n",
      " [-0.13616419]\n",
      " [-0.2398665 ]\n",
      " [-0.16200805]\n",
      " [-0.27050364]\n",
      " [-0.32797226]\n",
      " [-0.52210504]]\n",
      "12 Cost:  0.5641887 \n",
      "Prediction:\n",
      " [[-0.06526744]\n",
      " [-0.06751734]\n",
      " [-0.13613337]\n",
      " [-0.23984247]\n",
      " [-0.16197973]\n",
      " [-0.27047634]\n",
      " [-0.3279538 ]\n",
      " [-0.52208686]]\n",
      "13 Cost:  0.56414586 \n",
      "Prediction:\n",
      " [[-0.06522936]\n",
      " [-0.06748044]\n",
      " [-0.13610256]\n",
      " [-0.23981851]\n",
      " [-0.16195142]\n",
      " [-0.27044898]\n",
      " [-0.32793528]\n",
      " [-0.52206874]]\n",
      "14 Cost:  0.56410307 \n",
      "Prediction:\n",
      " [[-0.06519127]\n",
      " [-0.06744355]\n",
      " [-0.13607177]\n",
      " [-0.23979455]\n",
      " [-0.16192317]\n",
      " [-0.2704216 ]\n",
      " [-0.3279168 ]\n",
      " [-0.52205056]]\n",
      "15 Cost:  0.5640603 \n",
      "Prediction:\n",
      " [[-0.06515324]\n",
      " [-0.06740677]\n",
      " [-0.13604099]\n",
      " [-0.2397705 ]\n",
      " [-0.16189486]\n",
      " [-0.27039427]\n",
      " [-0.32789832]\n",
      " [-0.52203244]]\n",
      "16 Cost:  0.56401753 \n",
      "Prediction:\n",
      " [[-0.06511521]\n",
      " [-0.06736988]\n",
      " [-0.13601017]\n",
      " [-0.23974657]\n",
      " [-0.16186655]\n",
      " [-0.2703669 ]\n",
      " [-0.32787985]\n",
      " [-0.52201426]]\n",
      "17 Cost:  0.5639746 \n",
      "Prediction:\n",
      " [[-0.06507713]\n",
      " [-0.06733304]\n",
      " [-0.13597935]\n",
      " [-0.23972252]\n",
      " [-0.1618382 ]\n",
      " [-0.27033952]\n",
      " [-0.32786137]\n",
      " [-0.5219961 ]]\n",
      "18 Cost:  0.5639319 \n",
      "Prediction:\n",
      " [[-0.06503904]\n",
      " [-0.06729618]\n",
      " [-0.13594854]\n",
      " [-0.23969853]\n",
      " [-0.16180989]\n",
      " [-0.2703122 ]\n",
      " [-0.32784286]\n",
      " [-0.52197796]]\n",
      "19 Cost:  0.563889 \n",
      "Prediction:\n",
      " [[-0.06500101]\n",
      " [-0.06725931]\n",
      " [-0.13591772]\n",
      " [-0.2396746 ]\n",
      " [-0.16178155]\n",
      " [-0.27028477]\n",
      " [-0.32782435]\n",
      " [-0.5219598 ]]\n",
      "20 Cost:  0.5638463 \n",
      "Prediction:\n",
      " [[-0.06496298]\n",
      " [-0.06722248]\n",
      " [-0.13588694]\n",
      " [-0.23965058]\n",
      " [-0.16175324]\n",
      " [-0.2702574 ]\n",
      " [-0.32780588]\n",
      " [-0.52194166]]\n",
      "21 Cost:  0.56380343 \n",
      "Prediction:\n",
      " [[-0.0649249 ]\n",
      " [-0.06718558]\n",
      " [-0.13585609]\n",
      " [-0.23962665]\n",
      " [-0.16172493]\n",
      " [-0.2702301 ]\n",
      " [-0.3277874 ]\n",
      " [-0.5219235 ]]\n",
      "22 Cost:  0.56376064 \n",
      "Prediction:\n",
      " [[-0.06488681]\n",
      " [-0.06714875]\n",
      " [-0.13582528]\n",
      " [-0.2396026 ]\n",
      " [-0.16169661]\n",
      " [-0.2702027 ]\n",
      " [-0.32776892]\n",
      " [-0.5219053 ]]\n",
      "23 Cost:  0.5637179 \n",
      "Prediction:\n",
      " [[-0.06484878]\n",
      " [-0.06711191]\n",
      " [-0.13579446]\n",
      " [-0.2395786 ]\n",
      " [-0.1616683 ]\n",
      " [-0.27017534]\n",
      " [-0.32775044]\n",
      " [-0.5218872 ]]\n",
      "24 Cost:  0.56367517 \n",
      "Prediction:\n",
      " [[-0.06481075]\n",
      " [-0.06707507]\n",
      " [-0.1357637 ]\n",
      " [-0.23955467]\n",
      " [-0.16163999]\n",
      " [-0.27014798]\n",
      " [-0.32773197]\n",
      " [-0.521869  ]]\n",
      "25 Cost:  0.56363237 \n",
      "Prediction:\n",
      " [[-0.06477273]\n",
      " [-0.06703824]\n",
      " [-0.13573295]\n",
      " [-0.23953068]\n",
      " [-0.1616118 ]\n",
      " [-0.27012065]\n",
      " [-0.32771346]\n",
      " [-0.5218509 ]]\n",
      "26 Cost:  0.5635896 \n",
      "Prediction:\n",
      " [[-0.0647347 ]\n",
      " [-0.0670014 ]\n",
      " [-0.13570213]\n",
      " [-0.23950672]\n",
      " [-0.16158345]\n",
      " [-0.27009332]\n",
      " [-0.32769498]\n",
      " [-0.5218327 ]]\n",
      "27 Cost:  0.56354696 \n",
      "Prediction:\n",
      " [[-0.06469673]\n",
      " [-0.0669646 ]\n",
      " [-0.1356714 ]\n",
      " [-0.23948282]\n",
      " [-0.1615552 ]\n",
      " [-0.27006596]\n",
      " [-0.32767653]\n",
      " [-0.5218146 ]]\n",
      "28 Cost:  0.5635043 \n",
      "Prediction:\n",
      " [[-0.06465876]\n",
      " [-0.06692785]\n",
      " [-0.13564062]\n",
      " [-0.23945886]\n",
      " [-0.16152698]\n",
      " [-0.27003872]\n",
      " [-0.32765812]\n",
      " [-0.5217965 ]]\n",
      "29 Cost:  0.5634616 \n",
      "Prediction:\n",
      " [[-0.06462079]\n",
      " [-0.06689107]\n",
      " [-0.13560992]\n",
      " [-0.23943496]\n",
      " [-0.16149873]\n",
      " [-0.27001148]\n",
      " [-0.3276397 ]\n",
      " [-0.5217784 ]]\n",
      "30 Cost:  0.56341887 \n",
      "Prediction:\n",
      " [[-0.06458282]\n",
      " [-0.06685427]\n",
      " [-0.13557917]\n",
      " [-0.23941103]\n",
      " [-0.16147047]\n",
      " [-0.26998416]\n",
      " [-0.32762128]\n",
      " [-0.52176034]]\n",
      "31 Cost:  0.5633763 \n",
      "Prediction:\n",
      " [[-0.06454486]\n",
      " [-0.06681752]\n",
      " [-0.13554841]\n",
      " [-0.23938712]\n",
      " [-0.16144222]\n",
      " [-0.2699569 ]\n",
      " [-0.32760286]\n",
      " [-0.5217422 ]]\n",
      "32 Cost:  0.56333363 \n",
      "Prediction:\n",
      " [[-0.06450689]\n",
      " [-0.06678075]\n",
      " [-0.13551769]\n",
      " [-0.23936325]\n",
      " [-0.161414  ]\n",
      " [-0.26992953]\n",
      " [-0.32758445]\n",
      " [-0.52172416]]\n",
      "33 Cost:  0.56329095 \n",
      "Prediction:\n",
      " [[-0.06446892]\n",
      " [-0.06674394]\n",
      " [-0.13548696]\n",
      " [-0.23933929]\n",
      " [-0.16138577]\n",
      " [-0.2699023 ]\n",
      " [-0.32756603]\n",
      " [-0.52170604]]\n",
      "34 Cost:  0.5632483 \n",
      "Prediction:\n",
      " [[-0.06443095]\n",
      " [-0.06670719]\n",
      " [-0.13545623]\n",
      " [-0.23931539]\n",
      " [-0.16135752]\n",
      " [-0.26987505]\n",
      " [-0.3275476 ]\n",
      " [-0.5216879 ]]\n",
      "35 Cost:  0.5632056 \n",
      "Prediction:\n",
      " [[-0.06439298]\n",
      " [-0.06667039]\n",
      " [-0.13542548]\n",
      " [-0.23929143]\n",
      " [-0.16132924]\n",
      " [-0.2698477 ]\n",
      " [-0.32752916]\n",
      " [-0.52166986]]\n",
      "36 Cost:  0.56316304 \n",
      "Prediction:\n",
      " [[-0.06435502]\n",
      " [-0.06663367]\n",
      " [-0.13539481]\n",
      " [-0.23926756]\n",
      " [-0.16130108]\n",
      " [-0.26982042]\n",
      " [-0.32751074]\n",
      " [-0.52165174]]\n",
      "37 Cost:  0.5631203 \n",
      "Prediction:\n",
      " [[-0.06431705]\n",
      " [-0.06659687]\n",
      " [-0.13536406]\n",
      " [-0.23924366]\n",
      " [-0.16127285]\n",
      " [-0.26979312]\n",
      " [-0.32749233]\n",
      " [-0.5216337 ]]\n",
      "38 Cost:  0.5630777 \n",
      "Prediction:\n",
      " [[-0.06427908]\n",
      " [-0.06656009]\n",
      " [-0.13533336]\n",
      " [-0.2392197 ]\n",
      " [-0.16124457]\n",
      " [-0.26976585]\n",
      " [-0.32747388]\n",
      " [-0.52161556]]\n",
      "39 Cost:  0.563035 \n",
      "Prediction:\n",
      " [[-0.06424111]\n",
      " [-0.06652331]\n",
      " [-0.13530257]\n",
      " [-0.23919582]\n",
      " [-0.16121638]\n",
      " [-0.2697386 ]\n",
      " [-0.32745546]\n",
      " [-0.52159745]]\n",
      "40 Cost:  0.56299233 \n",
      "Prediction:\n",
      " [[-0.06420314]\n",
      " [-0.06648654]\n",
      " [-0.13527185]\n",
      " [-0.23917186]\n",
      " [-0.1611881 ]\n",
      " [-0.26971126]\n",
      " [-0.32743704]\n",
      " [-0.5215794 ]]\n",
      "41 Cost:  0.56294966 \n",
      "Prediction:\n",
      " [[-0.06416517]\n",
      " [-0.06644973]\n",
      " [-0.13524109]\n",
      " [-0.23914796]\n",
      " [-0.16115987]\n",
      " [-0.269684  ]\n",
      " [-0.32741863]\n",
      " [-0.52156126]]\n",
      "42 Cost:  0.5629071 \n",
      "Prediction:\n",
      " [[-0.06412721]\n",
      " [-0.06641299]\n",
      " [-0.13521034]\n",
      " [-0.23912406]\n",
      " [-0.16113162]\n",
      " [-0.26965672]\n",
      " [-0.3274002 ]\n",
      " [-0.5215432 ]]\n",
      "43 Cost:  0.5628644 \n",
      "Prediction:\n",
      " [[-0.06408924]\n",
      " [-0.06637621]\n",
      " [-0.13517964]\n",
      " [-0.23910013]\n",
      " [-0.16110337]\n",
      " [-0.2696294 ]\n",
      " [-0.3273818 ]\n",
      " [-0.5215251 ]]\n",
      "44 Cost:  0.56282175 \n",
      "Prediction:\n",
      " [[-0.06405127]\n",
      " [-0.0663394 ]\n",
      " [-0.13514888]\n",
      " [-0.2390762 ]\n",
      " [-0.16107512]\n",
      " [-0.26960215]\n",
      " [-0.32736337]\n",
      " [-0.52150697]]\n",
      "45 Cost:  0.56277907 \n",
      "Prediction:\n",
      " [[-0.0640133 ]\n",
      " [-0.06630266]\n",
      " [-0.13511813]\n",
      " [-0.23905233]\n",
      " [-0.16104689]\n",
      " [-0.26957482]\n",
      " [-0.32734495]\n",
      " [-0.5214889 ]]\n",
      "46 Cost:  0.5627365 \n",
      "Prediction:\n",
      " [[-0.06397533]\n",
      " [-0.06626588]\n",
      " [-0.1350874 ]\n",
      " [-0.2390284 ]\n",
      " [-0.1610187 ]\n",
      " [-0.26954752]\n",
      " [-0.3273265 ]\n",
      " [-0.5214708 ]]\n",
      "47 Cost:  0.5626939 \n",
      "Prediction:\n",
      " [[-0.06393737]\n",
      " [-0.06622913]\n",
      " [-0.1350567 ]\n",
      " [-0.2390045 ]\n",
      " [-0.16099048]\n",
      " [-0.26952028]\n",
      " [-0.3273081 ]\n",
      " [-0.5214527 ]]\n",
      "48 Cost:  0.5626512 \n",
      "Prediction:\n",
      " [[-0.0638994 ]\n",
      " [-0.06619233]\n",
      " [-0.13502598]\n",
      " [-0.23898053]\n",
      " [-0.16096222]\n",
      " [-0.26949295]\n",
      " [-0.32728967]\n",
      " [-0.5214346 ]]\n",
      "49 Cost:  0.5626086 \n",
      "Prediction:\n",
      " [[-0.06386143]\n",
      " [-0.06615555]\n",
      " [-0.13499522]\n",
      " [-0.23895663]\n",
      " [-0.16093397]\n",
      " [-0.2694657 ]\n",
      " [-0.32727125]\n",
      " [-0.52141654]]\n",
      "50 Cost:  0.562566 \n",
      "Prediction:\n",
      " [[-0.06382346]\n",
      " [-0.06611881]\n",
      " [-0.13496453]\n",
      " [-0.23893276]\n",
      " [-0.16090572]\n",
      " [-0.26943845]\n",
      " [-0.32725284]\n",
      " [-0.5213984 ]]\n",
      "51 Cost:  0.5625233 \n",
      "Prediction:\n",
      " [[-0.06378549]\n",
      " [-0.066082  ]\n",
      " [-0.13493377]\n",
      " [-0.2389088 ]\n",
      " [-0.1608775 ]\n",
      " [-0.2694111 ]\n",
      " [-0.3272344 ]\n",
      " [-0.5213803 ]]\n",
      "52 Cost:  0.5624807 \n",
      "Prediction:\n",
      " [[-0.06374753]\n",
      " [-0.06604522]\n",
      " [-0.13490307]\n",
      " [-0.2388849 ]\n",
      " [-0.16084921]\n",
      " [-0.26938382]\n",
      " [-0.32721597]\n",
      " [-0.52136225]]\n",
      "53 Cost:  0.562438 \n",
      "Prediction:\n",
      " [[-0.06370956]\n",
      " [-0.06600848]\n",
      " [-0.13487232]\n",
      " [-0.23886096]\n",
      " [-0.16082102]\n",
      " [-0.26935652]\n",
      " [-0.32719755]\n",
      " [-0.5213441 ]]\n",
      "54 Cost:  0.56239545 \n",
      "Prediction:\n",
      " [[-0.06367159]\n",
      " [-0.06597173]\n",
      " [-0.13484156]\n",
      " [-0.23883706]\n",
      " [-0.16079277]\n",
      " [-0.26932928]\n",
      " [-0.32717913]\n",
      " [-0.52132607]]\n",
      "55 Cost:  0.5623528 \n",
      "Prediction:\n",
      " [[-0.06363362]\n",
      " [-0.06593493]\n",
      " [-0.13481086]\n",
      " [-0.23881316]\n",
      " [-0.16076452]\n",
      " [-0.269302  ]\n",
      " [-0.32716072]\n",
      " [-0.52130795]]\n",
      "56 Cost:  0.5623102 \n",
      "Prediction:\n",
      " [[-0.06359565]\n",
      " [-0.06589824]\n",
      " [-0.13478011]\n",
      " [-0.2387892 ]\n",
      " [-0.16073626]\n",
      " [-0.26927468]\n",
      " [-0.3271423 ]\n",
      " [-0.5212899 ]]\n",
      "57 Cost:  0.56226754 \n",
      "Prediction:\n",
      " [[-0.06355768]\n",
      " [-0.06586143]\n",
      " [-0.13474947]\n",
      " [-0.23876533]\n",
      " [-0.16070813]\n",
      " [-0.2692474 ]\n",
      " [-0.32712388]\n",
      " [-0.5212718 ]]\n",
      "58 Cost:  0.562225 \n",
      "Prediction:\n",
      " [[-0.06351972]\n",
      " [-0.06582475]\n",
      " [-0.13471872]\n",
      " [-0.23874143]\n",
      " [-0.16067988]\n",
      " [-0.2692201 ]\n",
      " [-0.32710546]\n",
      " [-0.5212537 ]]\n",
      "59 Cost:  0.5621824 \n",
      "Prediction:\n",
      " [[-0.06348175]\n",
      " [-0.06578791]\n",
      " [-0.13468796]\n",
      " [-0.2387175 ]\n",
      " [-0.16065162]\n",
      " [-0.26919287]\n",
      " [-0.32708704]\n",
      " [-0.52123564]]\n",
      "60 Cost:  0.56213975 \n",
      "Prediction:\n",
      " [[-0.06344378]\n",
      " [-0.06575114]\n",
      " [-0.13465723]\n",
      " [-0.2386936 ]\n",
      " [-0.16062337]\n",
      " [-0.26916558]\n",
      " [-0.32706863]\n",
      " [-0.5212175 ]]\n",
      "61 Cost:  0.5620972 \n",
      "Prediction:\n",
      " [[-0.06340587]\n",
      " [-0.06571448]\n",
      " [-0.13462654]\n",
      " [-0.23866963]\n",
      " [-0.16059515]\n",
      " [-0.26913828]\n",
      " [-0.3270502 ]\n",
      " [-0.52119946]]\n",
      "62 Cost:  0.5620545 \n",
      "Prediction:\n",
      " [[-0.0633679 ]\n",
      " [-0.0656777 ]\n",
      " [-0.13459578]\n",
      " [-0.23864576]\n",
      " [-0.16056693]\n",
      " [-0.26911104]\n",
      " [-0.3270318 ]\n",
      " [-0.52118134]]\n",
      "63 Cost:  0.56201196 \n",
      "Prediction:\n",
      " [[-0.06332994]\n",
      " [-0.06564093]\n",
      " [-0.13456509]\n",
      " [-0.23862189]\n",
      " [-0.1605387 ]\n",
      " [-0.26908377]\n",
      " [-0.32701334]\n",
      " [-0.5211633 ]]\n",
      "64 Cost:  0.5619694 \n",
      "Prediction:\n",
      " [[-0.06329203]\n",
      " [-0.06560421]\n",
      " [-0.13453439]\n",
      " [-0.23859793]\n",
      " [-0.16051048]\n",
      " [-0.2690565 ]\n",
      " [-0.32699496]\n",
      " [-0.5211452 ]]\n",
      "65 Cost:  0.56192684 \n",
      "Prediction:\n",
      " [[-0.06325412]\n",
      " [-0.06556749]\n",
      " [-0.13450363]\n",
      " [-0.23857406]\n",
      " [-0.16048226]\n",
      " [-0.26902923]\n",
      " [-0.32697654]\n",
      " [-0.5211271 ]]\n",
      "66 Cost:  0.5618843 \n",
      "Prediction:\n",
      " [[-0.06321615]\n",
      " [-0.06553072]\n",
      " [-0.13447294]\n",
      " [-0.23855013]\n",
      " [-0.16045403]\n",
      " [-0.2690019 ]\n",
      " [-0.32695812]\n",
      " [-0.52110904]]\n",
      "67 Cost:  0.56184167 \n",
      "Prediction:\n",
      " [[-0.06317818]\n",
      " [-0.065494  ]\n",
      " [-0.13444221]\n",
      " [-0.23852623]\n",
      " [-0.16042578]\n",
      " [-0.26897466]\n",
      " [-0.32693967]\n",
      " [-0.5210909 ]]\n",
      "68 Cost:  0.56179917 \n",
      "Prediction:\n",
      " [[-0.06314027]\n",
      " [-0.06545722]\n",
      " [-0.13441154]\n",
      " [-0.23850235]\n",
      " [-0.16039765]\n",
      " [-0.26894742]\n",
      " [-0.32692125]\n",
      " [-0.52107286]]\n",
      "69 Cost:  0.56175655 \n",
      "Prediction:\n",
      " [[-0.06310236]\n",
      " [-0.06542051]\n",
      " [-0.13438085]\n",
      " [-0.23847842]\n",
      " [-0.1603694 ]\n",
      " [-0.26892012]\n",
      " [-0.32690284]\n",
      " [-0.52105474]]\n",
      "70 Cost:  0.56171393 \n",
      "Prediction:\n",
      " [[-0.0630644 ]\n",
      " [-0.06538379]\n",
      " [-0.13435015]\n",
      " [-0.23845452]\n",
      " [-0.1603412 ]\n",
      " [-0.26889285]\n",
      " [-0.32688442]\n",
      " [-0.5210367 ]]\n",
      "71 Cost:  0.5616714 \n",
      "Prediction:\n",
      " [[-0.06302643]\n",
      " [-0.06534702]\n",
      " [-0.13431942]\n",
      " [-0.23843062]\n",
      " [-0.16031295]\n",
      " [-0.26886556]\n",
      " [-0.32686597]\n",
      " [-0.5210186 ]]\n",
      "72 Cost:  0.5616288 \n",
      "Prediction:\n",
      " [[-0.06298852]\n",
      " [-0.0653103 ]\n",
      " [-0.1342887 ]\n",
      " [-0.23840672]\n",
      " [-0.16028476]\n",
      " [-0.26883832]\n",
      " [-0.32684755]\n",
      " [-0.5210005 ]]\n",
      "73 Cost:  0.56158626 \n",
      "Prediction:\n",
      " [[-0.06295061]\n",
      " [-0.06527352]\n",
      " [-0.134258  ]\n",
      " [-0.23838282]\n",
      " [-0.1602565 ]\n",
      " [-0.26881108]\n",
      " [-0.32682917]\n",
      " [-0.52098244]]\n",
      "74 Cost:  0.5615437 \n",
      "Prediction:\n",
      " [[-0.06291264]\n",
      " [-0.06523681]\n",
      " [-0.13422728]\n",
      " [-0.23835889]\n",
      " [-0.16022828]\n",
      " [-0.26878378]\n",
      " [-0.32681075]\n",
      " [-0.5209643 ]]\n",
      "75 Cost:  0.5615011 \n",
      "Prediction:\n",
      " [[-0.06287467]\n",
      " [-0.06520009]\n",
      " [-0.13419655]\n",
      " [-0.23833501]\n",
      " [-0.16020006]\n",
      " [-0.2687565 ]\n",
      " [-0.3267923 ]\n",
      " [-0.52094626]]\n",
      "76 Cost:  0.5614586 \n",
      "Prediction:\n",
      " [[-0.06283677]\n",
      " [-0.06516331]\n",
      " [-0.13416585]\n",
      " [-0.23831111]\n",
      " [-0.16017184]\n",
      " [-0.26872927]\n",
      " [-0.32677388]\n",
      " [-0.52092814]]\n",
      "77 Cost:  0.56141603 \n",
      "Prediction:\n",
      " [[-0.06279886]\n",
      " [-0.06512666]\n",
      " [-0.13413513]\n",
      " [-0.23828718]\n",
      " [-0.16014361]\n",
      " [-0.26870197]\n",
      " [-0.32675546]\n",
      " [-0.5209101 ]]\n",
      "78 Cost:  0.5613735 \n",
      "Prediction:\n",
      " [[-0.06276089]\n",
      " [-0.06508988]\n",
      " [-0.13410446]\n",
      " [-0.23826331]\n",
      " [-0.16011542]\n",
      " [-0.26867467]\n",
      " [-0.32673705]\n",
      " [-0.520892  ]]\n",
      "79 Cost:  0.5613309 \n",
      "Prediction:\n",
      " [[-0.06272292]\n",
      " [-0.06505311]\n",
      " [-0.13407376]\n",
      " [-0.23823935]\n",
      " [-0.16008723]\n",
      " [-0.26864743]\n",
      " [-0.32671863]\n",
      " [-0.5208739 ]]\n",
      "80 Cost:  0.56128836 \n",
      "Prediction:\n",
      " [[-0.06268501]\n",
      " [-0.06501639]\n",
      " [-0.13404304]\n",
      " [-0.2382155 ]\n",
      " [-0.16005898]\n",
      " [-0.26862013]\n",
      " [-0.3267002 ]\n",
      " [-0.52085584]]\n",
      "81 Cost:  0.5612458 \n",
      "Prediction:\n",
      " [[-0.0626471 ]\n",
      " [-0.06497967]\n",
      " [-0.13401231]\n",
      " [-0.2381916 ]\n",
      " [-0.16003075]\n",
      " [-0.2685929 ]\n",
      " [-0.3266818 ]\n",
      " [-0.5208377 ]]\n",
      "82 Cost:  0.56120324 \n",
      "Prediction:\n",
      " [[-0.06260914]\n",
      " [-0.0649429 ]\n",
      " [-0.13398162]\n",
      " [-0.23816764]\n",
      " [-0.16000253]\n",
      " [-0.2685656 ]\n",
      " [-0.32666337]\n",
      " [-0.52081966]]\n",
      "83 Cost:  0.5611607 \n",
      "Prediction:\n",
      " [[-0.06257117]\n",
      " [-0.06490618]\n",
      " [-0.13395089]\n",
      " [-0.23814374]\n",
      " [-0.15997428]\n",
      " [-0.26853836]\n",
      " [-0.32664496]\n",
      " [-0.52080154]]\n",
      "84 Cost:  0.5611181 \n",
      "Prediction:\n",
      " [[-0.06253326]\n",
      " [-0.0648694 ]\n",
      " [-0.13392016]\n",
      " [-0.2381199 ]\n",
      " [-0.15994608]\n",
      " [-0.26851106]\n",
      " [-0.32662654]\n",
      " [-0.5207835 ]]\n",
      "85 Cost:  0.56107557 \n",
      "Prediction:\n",
      " [[-0.06249535]\n",
      " [-0.06483269]\n",
      " [-0.13388947]\n",
      " [-0.23809594]\n",
      " [-0.15991783]\n",
      " [-0.2684838 ]\n",
      " [-0.32660812]\n",
      " [-0.5207654 ]]\n",
      "86 Cost:  0.561033 \n",
      "Prediction:\n",
      " [[-0.06245738]\n",
      " [-0.06479597]\n",
      " [-0.13385874]\n",
      " [-0.23807207]\n",
      " [-0.15988964]\n",
      " [-0.26845658]\n",
      " [-0.3265897 ]\n",
      " [-0.5207473 ]]\n",
      "87 Cost:  0.5609905 \n",
      "Prediction:\n",
      " [[-0.06241941]\n",
      " [-0.06475919]\n",
      " [-0.13382804]\n",
      " [-0.23804814]\n",
      " [-0.15986139]\n",
      " [-0.26842922]\n",
      " [-0.3265713 ]\n",
      " [-0.52072924]]\n",
      "88 Cost:  0.5609479 \n",
      "Prediction:\n",
      " [[-0.06238151]\n",
      " [-0.06472248]\n",
      " [-0.13379732]\n",
      " [-0.23802423]\n",
      " [-0.15983316]\n",
      " [-0.26840198]\n",
      " [-0.32655287]\n",
      " [-0.5207111 ]]\n",
      "89 Cost:  0.56090546 \n",
      "Prediction:\n",
      " [[-0.0623436 ]\n",
      " [-0.0646857 ]\n",
      " [-0.13376668]\n",
      " [-0.23800036]\n",
      " [-0.15980497]\n",
      " [-0.26837474]\n",
      " [-0.32653445]\n",
      " [-0.52069306]]\n",
      "90 Cost:  0.5608629 \n",
      "Prediction:\n",
      " [[-0.06230563]\n",
      " [-0.06464899]\n",
      " [-0.13373592]\n",
      " [-0.23797643]\n",
      " [-0.15977678]\n",
      " [-0.26834744]\n",
      " [-0.32651603]\n",
      " [-0.52067494]]\n",
      "91 Cost:  0.56082034 \n",
      "Prediction:\n",
      " [[-0.06226766]\n",
      " [-0.06461227]\n",
      " [-0.13370523]\n",
      " [-0.23795253]\n",
      " [-0.15974852]\n",
      " [-0.2683202 ]\n",
      " [-0.3264976 ]\n",
      " [-0.5206569 ]]\n",
      "92 Cost:  0.5607778 \n",
      "Prediction:\n",
      " [[-0.06222975]\n",
      " [-0.06457549]\n",
      " [-0.13367453]\n",
      " [-0.23792857]\n",
      " [-0.1597203 ]\n",
      " [-0.26829287]\n",
      " [-0.32647917]\n",
      " [-0.5206388 ]]\n",
      "93 Cost:  0.5607352 \n",
      "Prediction:\n",
      " [[-0.06219184]\n",
      " [-0.06453872]\n",
      " [-0.13364378]\n",
      " [-0.23790473]\n",
      " [-0.15969208]\n",
      " [-0.26826566]\n",
      " [-0.32646075]\n",
      " [-0.5206207 ]]\n",
      "94 Cost:  0.5606927 \n",
      "Prediction:\n",
      " [[-0.06215388]\n",
      " [-0.06450206]\n",
      " [-0.13361308]\n",
      " [-0.23788083]\n",
      " [-0.15966386]\n",
      " [-0.2682384 ]\n",
      " [-0.32644233]\n",
      " [-0.52060264]]\n",
      "95 Cost:  0.56065017 \n",
      "Prediction:\n",
      " [[-0.06211591]\n",
      " [-0.06446528]\n",
      " [-0.13358238]\n",
      " [-0.23785686]\n",
      " [-0.15963563]\n",
      " [-0.26821107]\n",
      " [-0.3264239 ]\n",
      " [-0.5205845 ]]\n",
      "96 Cost:  0.5606077 \n",
      "Prediction:\n",
      " [[-0.062078  ]\n",
      " [-0.06442851]\n",
      " [-0.13355163]\n",
      " [-0.237833  ]\n",
      " [-0.15960741]\n",
      " [-0.26818383]\n",
      " [-0.3264055 ]\n",
      " [-0.52056646]]\n",
      "97 Cost:  0.5605651 \n",
      "Prediction:\n",
      " [[-0.06204009]\n",
      " [-0.06439185]\n",
      " [-0.13352093]\n",
      " [-0.23780912]\n",
      " [-0.15957919]\n",
      " [-0.26815653]\n",
      " [-0.32638708]\n",
      " [-0.52054834]]\n",
      "98 Cost:  0.5605226 \n",
      "Prediction:\n",
      " [[-0.06200212]\n",
      " [-0.06435508]\n",
      " [-0.13349023]\n",
      " [-0.23778516]\n",
      " [-0.15955096]\n",
      " [-0.2681293 ]\n",
      " [-0.32636866]\n",
      " [-0.5205303 ]]\n",
      "99 Cost:  0.56048006 \n",
      "Prediction:\n",
      " [[-0.06196415]\n",
      " [-0.06431836]\n",
      " [-0.13345948]\n",
      " [-0.23776129]\n",
      " [-0.15952271]\n",
      " [-0.26810202]\n",
      " [-0.32635024]\n",
      " [-0.5205122 ]]\n",
      "100 Cost:  0.56043756 \n",
      "Prediction:\n",
      " [[-0.06192625]\n",
      " [-0.06428158]\n",
      " [-0.13342884]\n",
      " [-0.23773736]\n",
      " [-0.15949458]\n",
      " [-0.26807472]\n",
      " [-0.3263318 ]\n",
      " [-0.5204941 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n0 Cost: 0.15230925 \\nPrediction:\\n [[ 1.6346191 ]\\n [ 0.06613699]\\n [ 0.3500818 ]\\n [ 0.6707252 ]\\n [ 0.61130744]\\n [ 0.61464405]\\n [ 0.23171967]\\n [-0.1372836 ]]\\n1 Cost: 0.15230872 \\nPrediction:\\n [[ 1.634618  ]\\n [ 0.06613836]\\n [ 0.35008252]\\n [ 0.670725  ]\\n [ 0.6113076 ]\\n [ 0.6146443 ]\\n [ 0.23172   ]\\n [-0.13728246]]\\n...\\n99 Cost: 0.1522546 \\nPrediction:\\n [[ 1.6345041 ]\\n [ 0.06627947]\\n [ 0.35014683]\\n [ 0.670706  ]\\n [ 0.6113161 ]\\n [ 0.61466044]\\n [ 0.23175153]\\n [-0.13716647]]\\n100 Cost: 0.15225402## 코스트가 너무작게 작아짐 이건 학습이너무 느리게 이러지고 있고 이는 local에 빠진것을 의미  \\nPrediction:\\n [[ 1.6345029 \\n [ 0.06628093]\\n [ 0.35014752]\\n [ 0.67070574]\\n [ 0.61131614]\\n [ 0.6146606 ]\\n [ 0.23175186]\\n [-0.13716528]]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)##행기준인데 열로보겠다. \n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)#분모가 0이되면안되니깝 \n",
    "\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],###너무 크게 차이가 난다.!!! 값들이 이런경우 값이 밖으로 나가게된다.!!\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy)##이렇게 하면 어느방향으로 가더라도 발산하지않고 수렴하게되서 원하는 값을 얻을 수 있음\n",
    "#즉 데이터의 선처리가 매우 중요하다 .\n",
    "print(xy)\n",
    "\"\"\"\n",
    "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
    " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
    " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
    " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
    " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
    " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
    " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
    " [0.         0.07747099 0.5326087  0.         0.        ]]###minmax로 하게 되면 데이터의 결과가 이렇게 됨\n",
    "\"\"\"\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)###이번엔 학습률을 너무 낮게 매긴경우\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
    "\n",
    "'''\n",
    "0 Cost: 0.15230925 \n",
    "Prediction:\n",
    " [[ 1.6346191 ]\n",
    " [ 0.06613699]\n",
    " [ 0.3500818 ]\n",
    " [ 0.6707252 ]\n",
    " [ 0.61130744]\n",
    " [ 0.61464405]\n",
    " [ 0.23171967]\n",
    " [-0.1372836 ]]\n",
    "1 Cost: 0.15230872 \n",
    "Prediction:\n",
    " [[ 1.634618  ]\n",
    " [ 0.06613836]\n",
    " [ 0.35008252]\n",
    " [ 0.670725  ]\n",
    " [ 0.6113076 ]\n",
    " [ 0.6146443 ]\n",
    " [ 0.23172   ]\n",
    " [-0.13728246]]\n",
    "...\n",
    "99 Cost: 0.1522546 \n",
    "Prediction:\n",
    " [[ 1.6345041 ]\n",
    " [ 0.06627947]\n",
    " [ 0.35014683]\n",
    " [ 0.670706  ]\n",
    " [ 0.6113161 ]\n",
    " [ 0.61466044]\n",
    " [ 0.23175153]\n",
    " [-0.13716647]]\n",
    "100 Cost: 0.15225402## 코스트가 너무작게 작아짐 이건 학습이너무 느리게 이러지고 있고 이는 local에 빠진것을 의미  \n",
    "Prediction:\n",
    " [[ 1.6345029 \n",
    " [ 0.06628093]\n",
    " [ 0.35014752]\n",
    " [ 0.67070574]\n",
    " [ 0.61131614]\n",
    " [ 0.6146606 ]\n",
    " [ 0.23175186]\n",
    " [-0.13716528]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 3.057904737\n",
      "Epoch: 0002, Cost: 1.141675663\n",
      "Epoch: 0003, Cost: 0.909058112\n",
      "Epoch: 0004, Cost: 0.793338155\n",
      "Epoch: 0005, Cost: 0.719723701\n",
      "Epoch: 0006, Cost: 0.667118192\n",
      "Epoch: 0007, Cost: 0.627397971\n",
      "Epoch: 0008, Cost: 0.595497162\n",
      "Epoch: 0009, Cost: 0.569353908\n",
      "Epoch: 0010, Cost: 0.547884509\n",
      "Epoch: 0011, Cost: 0.529179786\n",
      "Epoch: 0012, Cost: 0.512841499\n",
      "Epoch: 0013, Cost: 0.498917209\n",
      "Epoch: 0014, Cost: 0.486648281\n",
      "Epoch: 0015, Cost: 0.474862862\n",
      "Learning finished\n",
      "Accuracy:  0.8918\n",
      "Label:  [3]\n",
      "Prediction:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nEpoch: 0001, Cost: 2.826302672\\nEpoch: 0002, Cost: 1.061668952\\nEpoch: 0003, Cost: 0.838061315\\nEpoch: 0004, Cost: 0.733232745\\nEpoch: 0005, Cost: 0.669279885\\nEpoch: 0006, Cost: 0.624611836\\nEpoch: 0007, Cost: 0.591160344\\nEpoch: 0008, Cost: 0.563868987\\nEpoch: 0009, Cost: 0.541745171\\nEpoch: 0010, Cost: 0.522673578\\nEpoch: 0011, Cost: 0.506782325\\nEpoch: 0012, Cost: 0.492447643\\nEpoch: 0013, Cost: 0.479955837\\nEpoch: 0014, Cost: 0.468893674\\nEpoch: 0015, Cost: 0.458703488\\nLearning finished\\nAccuracy:  0.8951\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10##10가지의 숫자를 분류하는 것 이기 때문에 10개 \n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])#784개의 데이터를 입력 (28 x 28)\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])##10개를 집너넣음\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)##softmax를 사용하겠다.\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))##cross entropy를 사용하겠다.!!\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)##cost 최소화하자\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))####y의값과 예측한 값과 같은지 \n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15####너무큰 데이터셋이기때문에 한번에 100개씩 학습을 시키자 ##15번돌겠다 \n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)##체데이터의 개수를 배치사이즈로 나눔 10000/100 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):##15번돌것\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)#베치로 처리하겠다 100개씩\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))##epoch 전체 데이터 셋을 한번 학습시키는것 !! 1 epoch!!\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}##테스트셋을 넘겨줌\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Epoch: 0001, Cost: 2.826302672\n",
    "Epoch: 0002, Cost: 1.061668952\n",
    "Epoch: 0003, Cost: 0.838061315\n",
    "Epoch: 0004, Cost: 0.733232745\n",
    "Epoch: 0005, Cost: 0.669279885\n",
    "Epoch: 0006, Cost: 0.624611836\n",
    "Epoch: 0007, Cost: 0.591160344\n",
    "Epoch: 0008, Cost: 0.563868987\n",
    "Epoch: 0009, Cost: 0.541745171\n",
    "Epoch: 0010, Cost: 0.522673578\n",
    "Epoch: 0011, Cost: 0.506782325\n",
    "Epoch: 0012, Cost: 0.492447643\n",
    "Epoch: 0013, Cost: 0.479955837\n",
    "Epoch: 0014, Cost: 0.468893674\n",
    "Epoch: 0015, Cost: 0.458703488\n",
    "Learning finished\n",
    "Accuracy:  0.8951\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
